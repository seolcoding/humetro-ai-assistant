{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 휴메트로 RAG 파이프라인 구축 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "HUMETRO_LOCAL_RAG_EVAL\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Any, Optional\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LangSmith 설정\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"HUMETRO_LOCAL_RAG_EVAL\")  # 프로젝트 이름 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 프로젝트 개요\n",
    "휴메트로 AI 어시스턴트는 한국 도시철도 역무 지식에 특화된 RAG(Retrieval-Augmented Generation) 시스템입니다. \n",
    "\n",
    "프로젝트의 목표는 다음과 같습니다.\n",
    "  - 도시철도 역무 관련 문서를 효과적으로 색인화하여 검색 가능한 지식베이스 구축\n",
    "  - 다양한 로컬 LLM 모델을 평가하여 최적의 성능 비교\n",
    "  - 한국어 도메인에 특화된 RAG 시스템 개발\n",
    "  - 객관적인 평가 방법론을 통한 모델 성능 측정\n",
    "\n",
    "이 노트북은 데이터 로드부터 모델 평가까지 전체 RAG 파이프라인 워크플로우를 담고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리\n",
    "### 원본 문서\n",
    "\n",
    "부산교통공사 운영직 교육 자료(공식, 비공식)\n",
    "  - 야 너두 역무전문가\n",
    "  - 역무지식 100제\n",
    "  - 일타 역무\n",
    "  - 직원 교육 표준자료\n",
    "\n",
    "### 문서 전처리\n",
    "\n",
    "  - 한글 문서를 마크다운 형식으로 변환   \n",
    "  - hwp5html 라이브러리 활용   \n",
    "  - hwp -> html -> markdown으로 변환\n",
    "\n",
    "### 문서 로드\n",
    "  - 아래의 `load_documents` 함수로 문서를 로드하고\n",
    "  - `split_documents` 함수로 문서를 분할합니다. (800토큰 기준, 오버랩 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:02<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 문서 수: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_documents(doc_dir: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    특정 디렉토리에서 마크다운 문서를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        doc_dir: 마크다운 문서가 있는 디렉토리 경로\n",
    "\n",
    "    Returns:\n",
    "        문서 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 마크다운 로더 생성\n",
    "        loader = DirectoryLoader(\n",
    "            doc_dir,\n",
    "            glob=\"*.md\",  # 현재폴더의 마크다운만 로드\n",
    "            loader_cls=UnstructuredMarkdownLoader,\n",
    "            show_progress=True,\n",
    "            recursive=False,  # 하위 디렉토리는 검색하지 않음\n",
    "        )\n",
    "\n",
    "        # 문서 로드\n",
    "        documents = loader.load()\n",
    "\n",
    "        print(f\"로드된 문서 수: {len(documents)}\")\n",
    "\n",
    "        # 파일 이름을 metadata에 추가\n",
    "        for doc in documents:\n",
    "            if \"source\" in doc.metadata:\n",
    "                doc.metadata[\"filename\"] = os.path.basename(doc.metadata[\"source\"])\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"문서 로드 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 문서 로드 실행\n",
    "doc_dir = Path(\"./datasets/final_docs\")\n",
    "documents = load_documents(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 100,\n",
    "    include_filename_in_content: bool = False,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    문서를 청크로 분할하면서 파일 이름 정보를 유지합니다.\n",
    "\n",
    "    Args:\n",
    "        documents: 분할할 문서 리스트\n",
    "        chunk_size: 각 청크의 크기(토큰 수)\n",
    "        chunk_overlap: 인접한 청크 간의 겹치는 토큰 수\n",
    "        include_filename_in_content: 파일 이름을 본문에도 포함할지 여부\n",
    "\n",
    "    Returns:\n",
    "        분할된 문서 청크 리스트\n",
    "    \"\"\"\n",
    "    # 파일 이름을 본문에 포함하는 경우, 새 문서 리스트 생성\n",
    "    if include_filename_in_content:\n",
    "        preprocessed_docs = []\n",
    "        for doc in documents:\n",
    "            filename = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "            new_content = f\"파일: {filename}\\n\\n{doc.page_content}\"\n",
    "\n",
    "            # 새 문서 생성 (메타데이터는 유지)\n",
    "            new_doc = Document(page_content=new_content, metadata=doc.metadata)\n",
    "            preprocessed_docs.append(new_doc)\n",
    "\n",
    "        # 처리할 문서를 전처리된 문서로 교체\n",
    "        documents = preprocessed_docs\n",
    "\n",
    "    # 토큰 기반 분할기 생성\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    # 문서 분할\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"원본 문서 수: {len(documents)}, 분할 후 청크 수: {len(splits)}\")\n",
    "\n",
    "    # 청크 길이 통계\n",
    "    lengths = [len(doc.page_content) for doc in splits]\n",
    "    if lengths:\n",
    "        print(\n",
    "            f\"청크 길이 - 평균: {sum(lengths) / len(lengths):.1f}, 최소: {min(lengths)}, 최대: {max(lengths)}\"\n",
    "        )\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "# 문서 분할 실행\n",
    "splits = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터 데이터베이스 구축\n",
    "  - retrieveral 과정의 semantic search 에 사용되는 벡터 데이터베이스 구축\n",
    "  - 범용적인 로컬 벡터 데이터베이스인 chroma 사용\n",
    "  - 임베딩 모델은 OpenAI 의 text-embedding-3-small 사용\n",
    "  - 로컬에 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model_name=\"text-embedding-3-small\"):\n",
    "    \"\"\"OpenAI 임베딩 모델을 생성합니다.\"\"\"\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def create_vectorstore(\n",
    "    splits, embeddings, persist_dir, collection_name=\"rag_documents\"\n",
    "):\n",
    "    \"\"\"문서 청크를 임베딩하여 Chroma 벡터스토어에 저장합니다.\"\"\"\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "\n",
    "        print(f\"기존 벡터스토어 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "        if splits:\n",
    "            vectorstore.add_documents(splits)\n",
    "            vectorstore.persist()\n",
    "            print(\n",
    "                f\"벡터스토어 업데이트 완료. 총 문서 수: {vectorstore._collection.count()}\"\n",
    "            )\n",
    "    else:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        vectorstore.persist()\n",
    "        print(f\"벡터스토어 생성 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_dir, embeddings, collection_name=\"rag_documents\"):\n",
    "    \"\"\"저장된 Chroma 벡터스토어를 로드합니다.\"\"\"\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(f\"벡터스토어를 찾을 수 없습니다: {persist_dir}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 임베딩 및 벡터스토어 생성\n",
    "embeddings = create_embeddings()\n",
    "persist_dir = \"vectorstore\"\n",
    "vectorstore = create_vectorstore(\n",
    "    splits=splits, embeddings=embeddings, persist_dir=persist_dir\n",
    ")\n",
    "\n",
    "# 벡터스토어 로드 테스트\n",
    "loaded_vectorstore = load_vectorstore(persist_dir, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가를 위한 합성 데이터셋 생성\n",
    "  - X_AI 의 Grok-3-beta 및 gpt-4o를 데이터 생성의 기반 LLM으로 활용(무료 크레딧 보유 & SOTA에 근접한 성능)\n",
    "  - RAG 시스템의 평가에 널리 사용되는 RAGAS 프레임워크를 활용해서 데이터셋 생성\n",
    "  - 싱글 홉 질문, 멀티 홉 질문을 7:3 비율로 약 500여개 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "from ragas.testset.synthesizers.multi_hop.specific import (\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def generate_qa_dataset(\n",
    "    splits, output_path=\"datasets/synthetic_qa_dataset.csv\", num_questions=200\n",
    "):\n",
    "    \"\"\"\n",
    "    RAGAS를 사용하여 합성 QA 데이터셋을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        splits: 내용에서 질문을 생성할 Document 객체 목록\n",
    "        output_path: CSV 데이터셋을 저장할 경로\n",
    "        num_questions: 생성할 질문 수\n",
    "\n",
    "    Returns:\n",
    "        생성된 데이터셋이 담긴 Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # LLM 및 임베딩 래퍼 초기화\n",
    "    llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "    embeddings = LangchainEmbeddingsWrapper(\n",
    "        OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # TestsetGenerator 생성\n",
    "    generator = TestsetGenerator(llm=llm, embedding_model=embeddings)\n",
    "\n",
    "    # 질문 유형 분포 설정 (SingleHop 70%, MultiHop 30%)\n",
    "    distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=llm), 0.7),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=llm), 0.3),\n",
    "    ]\n",
    "\n",
    "    # 한국어로 질문과 답변 생성하도록 프롬프트 변경\n",
    "    for query, _ in distribution:\n",
    "        prompts = await query.adapt_prompts(\n",
    "            \"## 매우 중요: **한국어로만 질문과 답변을 생성**, Question and Answer MUST be in KOREAN\",\n",
    "            llm=llm,\n",
    "        )\n",
    "        query.set_prompts(**prompts)\n",
    "\n",
    "    # 테스트셋 생성\n",
    "    print(f\"총 {num_questions}개의 QA 쌍 생성 중...\")\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=splits,\n",
    "        testset_size=num_questions,\n",
    "        query_distribution=distribution,\n",
    "    )\n",
    "\n",
    "    # DataFrame으로 변환\n",
    "    test_df = testset.to_pandas()\n",
    "\n",
    "    # CSV로 저장\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    test_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"QA 데이터셋 생성 완료: {len(test_df)}개의 질문-답변 쌍\")\n",
    "    print(f\"{output_path}에 저장됨\")\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "test_df = asyncio.run(generate_qa_dataset(splits, num_questions=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 영어 데이터셋 한국어 번역\n",
    "- 생성된 영어 데이터셋을 한국어로 번역하여 도메인에 특화된 평가 데이터셋을 만듭니다.\n",
    "- RAGAS 프레임워크의 내부 프롬프트로 인해 생성된 영어로 된 질문을 한국어로 배치 번역\n",
    "- gpt-4o-mini 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
