{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 휴메트로 RAG 파이프라인 구축 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "HUMETRO_LOCAL_RAG_EVAL\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Any, Optional\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LangSmith 설정\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"HUMETRO_LOCAL_RAG_EVAL\")  # 프로젝트 이름 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 프로젝트 개요\n",
    "휴메트로 AI 어시스턴트는 한국 도시철도 역무 지식에 특화된 RAG(Retrieval-Augmented Generation) 시스템입니다. \n",
    "\n",
    "프로젝트의 목표는 다음과 같습니다.\n",
    "  - 도시철도 역무 관련 문서를 효과적으로 색인화하여 검색 가능한 지식베이스 구축\n",
    "  - 다양한 로컬 LLM 모델을 평가하여 최적의 성능 비교\n",
    "  - 한국어 도메인에 특화된 RAG 시스템 개발\n",
    "  - 객관적인 평가 방법론을 통한 모델 성능 측정\n",
    "\n",
    "이 노트북은 데이터 로드부터 모델 평가까지 전체 RAG 파이프라인 워크플로우를 담고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리\n",
    "### 원본 문서\n",
    "\n",
    "부산교통공사 운영직 교육 자료(공식, 비공식)\n",
    "  - 야 너두 역무전문가\n",
    "  - 역무지식 100제\n",
    "  - 일타 역무\n",
    "  - 직원 교육 표준자료\n",
    "\n",
    "### 문서 전처리\n",
    "\n",
    "  - 한글 문서를 마크다운 형식으로 변환   \n",
    "  - hwp5html 라이브러리 활용   \n",
    "  - hwp -> html -> markdown으로 변환\n",
    "\n",
    "### 문서 로드\n",
    "  - 아래의 `load_documents` 함수로 문서를 로드하고\n",
    "  - `split_documents` 함수로 문서를 분할합니다. (800토큰 기준, 오버랩 100)\n",
    "  - k=4, min_context_length = 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:02<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 문서 수: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_documents(doc_dir: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    특정 디렉토리에서 마크다운 문서를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        doc_dir: 마크다운 문서가 있는 디렉토리 경로\n",
    "\n",
    "    Returns:\n",
    "        문서 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 마크다운 로더 생성\n",
    "        loader = DirectoryLoader(\n",
    "            doc_dir,\n",
    "            glob=\"*.md\",  # 현재폴더의 마크다운만 로드\n",
    "            loader_cls=UnstructuredMarkdownLoader,\n",
    "            show_progress=True,\n",
    "            recursive=False,  # 하위 디렉토리는 검색하지 않음\n",
    "        )\n",
    "\n",
    "        # 문서 로드\n",
    "        documents = loader.load()\n",
    "\n",
    "        print(f\"로드된 문서 수: {len(documents)}\")\n",
    "\n",
    "        # 파일 이름을 metadata에 추가\n",
    "        for doc in documents:\n",
    "            if \"source\" in doc.metadata:\n",
    "                doc.metadata[\"filename\"] = os.path.basename(doc.metadata[\"source\"])\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"문서 로드 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 문서 로드 실행\n",
    "doc_dir = Path(\"./datasets/final_docs\")\n",
    "documents = load_documents(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 100,\n",
    "    include_filename_in_content: bool = False,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    문서를 청크로 분할하면서 파일 이름 정보를 유지합니다.\n",
    "\n",
    "    Args:\n",
    "        documents: 분할할 문서 리스트\n",
    "        chunk_size: 각 청크의 크기(토큰 수)\n",
    "        chunk_overlap: 인접한 청크 간의 겹치는 토큰 수\n",
    "        include_filename_in_content: 파일 이름을 본문에도 포함할지 여부\n",
    "\n",
    "    Returns:\n",
    "        분할된 문서 청크 리스트\n",
    "    \"\"\"\n",
    "    # 파일 이름을 본문에 포함하는 경우, 새 문서 리스트 생성\n",
    "    if include_filename_in_content:\n",
    "        preprocessed_docs = []\n",
    "        for doc in documents:\n",
    "            filename = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "            new_content = f\"파일: {filename}\\n\\n{doc.page_content}\"\n",
    "\n",
    "            # 새 문서 생성 (메타데이터는 유지)\n",
    "            new_doc = Document(page_content=new_content, metadata=doc.metadata)\n",
    "            preprocessed_docs.append(new_doc)\n",
    "\n",
    "        # 처리할 문서를 전처리된 문서로 교체\n",
    "        documents = preprocessed_docs\n",
    "\n",
    "    # 토큰 기반 분할기 생성\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    # 문서 분할\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"원본 문서 수: {len(documents)}, 분할 후 청크 수: {len(splits)}\")\n",
    "\n",
    "    # 청크 길이 통계\n",
    "    lengths = [len(doc.page_content) for doc in splits]\n",
    "    if lengths:\n",
    "        print(\n",
    "            f\"청크 길이 - 평균: {sum(lengths) / len(lengths):.1f}, 최소: {min(lengths)}, 최대: {max(lengths)}\"\n",
    "        )\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "# 문서 분할 실행\n",
    "splits = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터 데이터베이스 구축\n",
    "  - retrieveral 과정의 semantic search 에 사용되는 벡터 데이터베이스 구축\n",
    "  - 범용적인 로컬 벡터 데이터베이스인 chroma 사용\n",
    "  - 임베딩 모델은 OpenAI 의 text-embedding-3-small 사용\n",
    "  - 로컬에 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model_name=\"text-embedding-3-small\"):\n",
    "    \"\"\"OpenAI 임베딩 모델을 생성합니다.\"\"\"\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def create_vectorstore(\n",
    "    splits, embeddings, persist_dir, collection_name=\"rag_documents\"\n",
    "):\n",
    "    \"\"\"문서 청크를 임베딩하여 Chroma 벡터스토어에 저장합니다.\"\"\"\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "\n",
    "        print(f\"기존 벡터스토어 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "        if splits:\n",
    "            vectorstore.add_documents(splits)\n",
    "            vectorstore.persist()\n",
    "            print(\n",
    "                f\"벡터스토어 업데이트 완료. 총 문서 수: {vectorstore._collection.count()}\"\n",
    "            )\n",
    "    else:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        vectorstore.persist()\n",
    "        print(f\"벡터스토어 생성 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_dir, embeddings, collection_name=\"rag_documents\"):\n",
    "    \"\"\"저장된 Chroma 벡터스토어를 로드합니다.\"\"\"\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(f\"벡터스토어를 찾을 수 없습니다: {persist_dir}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 임베딩 및 벡터스토어 생성\n",
    "embeddings = create_embeddings()\n",
    "persist_dir = \"vectorstore\"\n",
    "vectorstore = create_vectorstore(\n",
    "    splits=splits, embeddings=embeddings, persist_dir=persist_dir\n",
    ")\n",
    "\n",
    "# 벡터스토어 로드 테스트\n",
    "loaded_vectorstore = load_vectorstore(persist_dir, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가를 위한 합성 데이터셋 생성\n",
    "  - X_AI 의 Grok-3-beta 및 gpt-4o를 데이터 생성의 기반 LLM으로 활용(무료 크레딧 보유 & SOTA에 근접한 성능)\n",
    "  - RAG 시스템의 평가에 널리 사용되는 RAGAS 프레임워크를 활용해서 데이터셋 생성\n",
    "  - 싱글 홉 질문, 멀티 홉 질문을 7:3 비율로 약 500여개 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "from ragas.testset.synthesizers.multi_hop.specific import (\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def generate_qa_dataset(\n",
    "    splits, output_path=\"datasets/synthetic_qa_dataset.csv\", num_questions=200\n",
    "):\n",
    "    \"\"\"\n",
    "    RAGAS를 사용하여 합성 QA 데이터셋을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        splits: 내용에서 질문을 생성할 Document 객체 목록\n",
    "        output_path: CSV 데이터셋을 저장할 경로\n",
    "        num_questions: 생성할 질문 수\n",
    "\n",
    "    Returns:\n",
    "        생성된 데이터셋이 담긴 Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # LLM 및 임베딩 래퍼 초기화\n",
    "    llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "    embeddings = LangchainEmbeddingsWrapper(\n",
    "        OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # TestsetGenerator 생성\n",
    "    generator = TestsetGenerator(llm=llm, embedding_model=embeddings)\n",
    "\n",
    "    # 질문 유형 분포 설정 (SingleHop 70%, MultiHop 30%)\n",
    "    distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=llm), 0.7),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=llm), 0.3),\n",
    "    ]\n",
    "\n",
    "    # 한국어로 질문과 답변 생성하도록 프롬프트 변경\n",
    "    for query, _ in distribution:\n",
    "        prompts = await query.adapt_prompts(\n",
    "            \"## 매우 중요: **한국어로만 질문과 답변을 생성**, Question and Answer MUST be in KOREAN\",\n",
    "            llm=llm,\n",
    "        )\n",
    "        query.set_prompts(**prompts)\n",
    "\n",
    "    # 테스트셋 생성\n",
    "    print(f\"총 {num_questions}개의 QA 쌍 생성 중...\")\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=splits,\n",
    "        testset_size=num_questions,\n",
    "        query_distribution=distribution,\n",
    "    )\n",
    "\n",
    "    # DataFrame으로 변환\n",
    "    test_df = testset.to_pandas()\n",
    "\n",
    "    # CSV로 저장\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    test_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"QA 데이터셋 생성 완료: {len(test_df)}개의 질문-답변 쌍\")\n",
    "    print(f\"{output_path}에 저장됨\")\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "test_df = asyncio.run(generate_qa_dataset(splits, num_questions=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 영어 데이터셋 한국어 번역\n",
    "- 생성된 영어 데이터셋을 한국어로 번역하여 도메인에 특화된 평가 데이터셋을 만듭니다.\n",
    "- RAGAS 프레임워크의 내부 프롬프트로 인해 생성된 영어로 된 질문을 한국어로 배치 번역\n",
    "- gpt-4o-mini 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 평가 데이터 생성을 위한 RAG chain 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 체인의 구성요소\n",
    " - local models \n",
    "    - params <= 4B (int8 quant VRAM <= 32GB>, FP16 VRAM <= 64GB)\n",
    "    - \"hyperclovax-seed-text-instruct-1.5b-hf-i1\",\n",
    "    - \"kakaocorp.kanana-nano-2.1b-instruct\",\n",
    "    - \"exaone-3.5-2.4b-instruct\",\n",
    "    - \"qwen/qwen3-4b:free\",\n",
    "    - \"google/gemma-3-4b-it:free\"  \n",
    "\n",
    " - bigger models  \n",
    "    - \"deepseek/deepseek-chat-v3-0324:free\", # 685B, OpenSource\n",
    "    - \"gpt-4o-mini\", # Baseline, OpenAI, Proprietary Model\n",
    " - retriever\n",
    "    - chroma vector database\n",
    "    - OpenAI text-embedding-3-small\n",
    "    - Naive RAG applied\n",
    " - rag chain\n",
    "    - langchain\n",
    "    - retriever\n",
    "    - simple RAG Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2수행과정\n",
    "1. 합성 데이터셋의 질문을 retriever에 넣어서 컨텍스트를 받아오고\n",
    "2. 질문과 컨텍스트를 모델에 넣어 답변을 생성하는 과정\n",
    "3. 생성한 답변 데이터는 추후 평가에 활용됨\n",
    "4. gpt-4o-mini 모델은 유사 태스크에 범용적으로 활용되는 모델로서 베이스라인 평가 척도로 활용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 체인 구축 및 답변생성 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_teddynote import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.langsmith(\"HUMETRO_EVAL_TEST\")\n",
    "\n",
    "\n",
    "def create_embeddings(model_name: str = \"text-embedding-3-small\") -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_directory: str, embeddings: Any) -> Chroma:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_documents\",\n",
    "    )\n",
    "    print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def create_retriever(vectorstore: Chroma, k: int = 4) -> Any:\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "def create_llm(model_signature: str, temperature: float = 0.1) -> Any:\n",
    "    OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "    local_models = [\n",
    "        \"exaone-3.5-2.4b-instruct\",\n",
    "        \"kakaocorp.kanana-nano-2.1b-instruct\",\n",
    "        \"hyperclovax-seed-text-instruct-1.5b-hf-i1\",\n",
    "    ]\n",
    "    open_router_models = [\n",
    "        \"qwen/qwen3-4b:free\",\n",
    "        \"google/gemma-3-4b-it:free\",\n",
    "        \"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    ]\n",
    "    if \"gpt\" in model_signature:\n",
    "        return ChatOpenAI(model=model_signature, temperature=temperature)\n",
    "    if model_signature in local_models:\n",
    "        try:\n",
    "            subprocess.run([\"lms\", \"unload\", \"-a\"])\n",
    "            print(\"모든 lms 언로드 완료\")\n",
    "            subprocess.run([\"lms\", \"load\", model_signature])\n",
    "            print(f\"lms 모델 {model_signature} 로드 완료\")\n",
    "            return ChatOpenAI(\n",
    "                base_url=\"http://localhost:1234/v1\", model=model_signature\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"lms 명령 실행 중 오류 발생: {e}\")\n",
    "            print(\"lms가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "    if model_signature in open_router_models:\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "            openai_api_base=OPENROUTER_BASE_URL,\n",
    "            model_name=model_signature,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return llm\n",
    "    return None\n",
    "\n",
    "\n",
    "# 5. RAG 체인 생성 함수\n",
    "def create_rag_chain(llm: Any, retriever: Any) -> Any:\n",
    "    # 한국의 역무환경을 고려한 RAG 프롬프트 템플릿\n",
    "    template = \"\"\"\n",
    "당신은 한국의 도시철도 역무 지식 도우미입니다.\n",
    "주어진 질문에 대해 제공된 문맥 정보를 기반으로 정확하고 도움이 되는 답변을 제공하세요.\n",
    "문맥에 없는 내용은 답변하지 마세요. 모르는 경우 솔직히 모른다고 말하세요.\n",
    "\n",
    "문맥 정보:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # LCEL을 사용한 RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    filename.replace(\".json\", \"\")\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.]\", \"_\", filename)[:30] + \".json\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_models = [\n",
    "        \"qwen/qwen3-4b:free\",\n",
    "        \"google/gemma-3-4b-it:free\",\n",
    "        \"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        \"hyperclovax-seed-text-instruct-1.5b-hf-i1\",\n",
    "        \"kakaocorp.kanana-nano-2.1b-instruct\",\n",
    "        \"exaone-3.5-2.4b-instruct\",\n",
    "        \"gpt-4o-mini\",\n",
    "    ]\n",
    "    embeddings = create_embeddings()\n",
    "    vectorstore = load_vectorstore(\n",
    "        persist_directory=\"vectorstore\", embeddings=embeddings\n",
    "    )\n",
    "    if len(vectorstore.similarity_search(\"서울역 주변 명소\")) == 0:\n",
    "        raise ValueError(\"vectorstore is empty\")\n",
    "    if len(vectorstore.get()[\"ids\"]) == 0:\n",
    "        raise ValueError(\"vectorstore is empty\")\n",
    "\n",
    "    # 질문 데이터 불러오기\n",
    "    question_data = pd.read_csv(\"./dataset_200.csv\")\n",
    "    base_questions = list(question_data[\"user_input\"])\n",
    "    print(base_questions[:10])\n",
    "\n",
    "    for model_signature in target_models:\n",
    "        try:\n",
    "            print(f\"Loading {model_signature}...\")\n",
    "            llm = create_llm(model_signature)\n",
    "            retriever = create_retriever(vectorstore)\n",
    "            rag_chain = create_rag_chain(llm, retriever)\n",
    "            filename = sanitize_filename(f\"result_{model_signature}.json\")\n",
    "            try:\n",
    "                with open(filename, \"r\") as f:\n",
    "                    result_list = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                result_list = []\n",
    "\n",
    "            # 이미 처리된 질문 제외\n",
    "            processed_question = [i[\"question\"] for i in result_list]\n",
    "            target_questions = [\n",
    "                i for i in base_questions if i not in processed_question\n",
    "            ]\n",
    "\n",
    "            for question in tqdm(\n",
    "                target_questions, desc=f\"Evaluating {model_signature}\"\n",
    "            ):\n",
    "                try:\n",
    "                    result = rag_chain.invoke(question)\n",
    "                    result_list.append({\"question\": question, \"answer\": result})\n",
    "                    if len(result_list) % 10 == 0:  # 10개마다 저장하기\n",
    "                        print(f\"checkpoint, saved {len(result_list)}\")\n",
    "                    with open(filename, \"w\") as f:\n",
    "                        json.dump(result_list, f, ensure_ascii=False)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error: while generating {model_signature} single question {question}: {e}\"\n",
    "                    )\n",
    "                    continue\n",
    "            with open(filename, \"w\") as f:  # 최종 결과 저장하기\n",
    "                json.dump(result_list, f, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: while generating {model_signature}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 답변 생성 결과\n",
    "langsmith 플랫폼을 활용하여 모니터링\n",
    "\n",
    "https://smith.langchain.com/o/06d0a329-b301-5d27-b5a2-94923e9bb6c0/projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAGAS의 기본 평가 메트릭을 적용한 평가 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 생성 데이터 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
