{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "HUMETRO_LOCAL_RAG_EVAL_TEST\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"HUMETRO_LOCAL_RAG_EVAL_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Any\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# 2. 임베딩 모델 생성\n",
    "def create_embeddings(model_name: str = \"text-embedding-3-small\") -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_directory: str, embeddings: Any) -> Chroma:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_documents\",\n",
    "    )\n",
    "    print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# 3. 검색기(retriever) 생성 함수\n",
    "def create_retriever(vectorstore: Chroma, k: int = 4) -> Any:\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "# 4. LLM 모델 생성 함수\n",
    "def create_llm_ollama(model_name: str, temperature: float = 0.1) -> Any:\n",
    "    if \"ollama\" in model_name:\n",
    "        # Ollama 모델 확인 및 없으면 다운로드\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"ps\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            running_servers = result.stdout.strip()\n",
    "\n",
    "            # 2. 실행 중인 모델이 있으면 종료\n",
    "            if running_servers:\n",
    "                print(\"현재 실행 중인 모델을 종료합니다...\")\n",
    "                for line in running_servers.split(\"\\n\"):\n",
    "                    if line and not line.startswith(\"NAME\"):  # 헤더 행 제외\n",
    "                        running_model = line.split()[0]\n",
    "                        # 실행 중인 모델이 요청된 모델과 다른 경우에만 종료\n",
    "                        if running_model != model_name:\n",
    "                            print(f\"모델 {running_model}을 종료합니다...\")\n",
    "                            subprocess.run(\n",
    "                                [\"ollama\", \"stop\", running_model],\n",
    "                                check=True,\n",
    "                                capture_output=True,\n",
    "                            )\n",
    "            # 사용 가능한 모델 목록 확인\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"list\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            available_models = result.stdout.lower()\n",
    "\n",
    "            # 모델 이름에서 태그 분리\n",
    "            if \":\" in model_name:\n",
    "                base_model = model_name.split(\":\")[0]\n",
    "            else:\n",
    "                base_model = model_name\n",
    "\n",
    "            # 모델이 없으면 다운로드\n",
    "            if base_model not in available_models:\n",
    "                print(f\"모델 {model_name}을 다운로드합니다...\")\n",
    "                subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "                print(f\"모델 {model_name} 다운로드 완료\")\n",
    "            else:\n",
    "                print(f\"모델 {model_name}이 이미 존재합니다.\")\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Ollama 명령 실행 중 오류 발생: {e}\")\n",
    "            print(\"Ollama가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama 모델 준비 중 오류 발생: {e}\")\n",
    "\n",
    "        # Ollama 모델 연결\n",
    "        return Ollama(model=model_name, temperature=temperature)\n",
    "    else:\n",
    "        # OpenAI 모델 사용\n",
    "        return ChatOpenAI(model=model_name, temperature=temperature)\n",
    "\n",
    "\n",
    "def create_llm_lms(model_signature: str, temperature: float = 0.1) -> Any:\n",
    "    if \"gpt\" in model_signature:\n",
    "        return ChatOpenAI(model=model_signature, temperature=temperature)\n",
    "    try:\n",
    "        subprocess.run([\"lms\", \"unload\", \"-a\"])\n",
    "        print(\"모든 lms 언로드 완료\")\n",
    "        subprocess.run([\"lms\", \"load\", model_signature])\n",
    "        print(f\"lms 모델 {model_signature} 로드 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"lms 명령 실행 중 오류 발생: {e}\")\n",
    "        print(\"lms가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "    return ChatOpenAI(base_url=\"http://localhost:1234/v1\", model=model_signature)\n",
    "\n",
    "\n",
    "# 5. RAG 체인 생성 함수\n",
    "def create_rag_chain(llm: Any, retriever: Any) -> Any:\n",
    "    # 한국의 역무환경을 고려한 RAG 프롬프트 템플릿\n",
    "    template = \"\"\"\n",
    "당신은 한국의 도시철도 역무 지식 도우미입니다.\n",
    "주어진 질문에 대해 제공된 문맥 정보를 기반으로 정확하고 도움이 되는 답변을 제공하세요.\n",
    "문맥에 없는 내용은 답변하지 마세요. 모르는 경우 솔직히 모른다고 말하세요.\n",
    "\n",
    "문맥 정보:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # LCEL을 사용한 RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터스토어 로드 완료. 문서 수: 150\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = create_embeddings()\n",
    "vectorstore = load_vectorstore(persist_directory=\"vectorstore\", embeddings=embeddings)\n",
    "if len(vectorstore.similarity_search(\"서울역 주변 명소\")) == 0:\n",
    "    raise ValueError(\"vectorstore is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exaone-3.5-2.4b-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading \"qwen3-1.7b\"...\n",
      "Unloaded 1 model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 lms 언로드 완료\n",
      "\n",
      "⠹ [▋                                                 ] 1.00%          \u001b[u\u001b[?25l\u001b[s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model \"lmstudio-community/EXAONE-3.5-2.4B-Instruct-GGUF/EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ [███▊                                              ] 7.47%          \u001b[u\u001b[2K\u001b[1G\u001b[?25hlms 모델 exaone-3.5-2.4b-instruct 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 547.00ms. (1.64 GB)\n",
      "To use the model in the API/SDK, use the identifier \"\u001b[92mexaone-3.5-2.4b-instruct\u001b[39m\".\n",
      "To set a custom identifier, use the \u001b[93m--identifier <identifier>\u001b[39m option.\n",
      "Evaluating exaone-3.5-2.4b-instruct: 100%|██████████| 3/3 [00:22<00:00,  7.63s/it]\n",
      "Unloading \"exaone-3.5-2.4b-instruct\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading kakaocorp.kanana-nano-2.1b-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloaded 1 model.\n",
      "Loading model \"DevQuasar/kakaocorp.kanana-nano-2.1b-instruct-GGUF/kakaocorp.kanana-nano-2.1b-instruct.Q6_K.gguf\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 lms 언로드 완료\n",
      "\n",
      "⠋ [███▍                                              ] 6.60%          \u001b[u\u001b[?25l\u001b[s\u001b[?25l\u001b[s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 642.00ms. (1.83 GB)\n",
      "To use the model in the API/SDK, use the identifier \"\u001b[92mkakaocorp.kanana-nano-2.1b-instruct\u001b[39m\".\n",
      "To set a custom identifier, use the \u001b[93m--identifier <identifier>\u001b[39m option.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ [████▎                                             ] 8.33%          \u001b[u\u001b[2K\u001b[1G\u001b[?25hlms 모델 kakaocorp.kanana-nano-2.1b-instruct 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating kakaocorp.kanana-nano-2.1b-instruct: 100%|██████████| 3/3 [00:20<00:00,  6.84s/it]\n",
      "Unloading \"kakaocorp.kanana-nano-2.1b-instruct\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading hyperclovax-seed-text-instruct-1.5b-hf-i1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloaded 1 model.\n",
      "Loading model \"mradermacher/HyperCLOVAX-SEED-Text-Instruct-1.5B-hf-i1-GGUF/HyperCLOVAX-SEED-Text-Instruct-1.5B-hf.i1-Q6_K.gguf\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 lms 언로드 완료\n",
      "\n",
      "⠋ [███▍                                              ] 6.60%          \u001b[u\u001b[2K\u001b[1G\u001b[?25hl\u001b[s\u001b[?25l\u001b[s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 546.00ms. (1.31 GB)\n",
      "To use the model in the API/SDK, use the identifier \"\u001b[92mhyperclovax-seed-text-instruct-1.5b-hf-i1\u001b[39m\".\n",
      "To set a custom identifier, use the \u001b[93m--identifier <identifier>\u001b[39m option.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lms 모델 hyperclovax-seed-text-instruct-1.5b-hf-i1 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hyperclovax-seed-text-instruct-1.5b-hf-i1: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "Unloading \"hyperclovax-seed-text-instruct-1.5b-hf-i1\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading qwen3-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloaded 1 model.\n",
      "Loading model \"lmstudio-community/Qwen3-4B-GGUF/Qwen3-4B-Q4_K_M.gguf\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 lms 언로드 완료\n",
      "\n",
      "⠋ [███▍                                              ] 6.60%          \u001b[u\u001b[?25l\u001b[s\u001b[?25l\u001b[s\u001b[?25l\u001b[s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 703.00ms. (2.50 GB)\n",
      "To use the model in the API/SDK, use the identifier \"\u001b[92mqwen3-4b\u001b[39m\".\n",
      "To set a custom identifier, use the \u001b[93m--identifier <identifier>\u001b[39m option.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ [████▎                                             ] 8.33%          \u001b[u\u001b[2K\u001b[1G\u001b[?25hlms 모델 qwen3-4b 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating qwen3-4b: 100%|██████████| 3/3 [00:54<00:00, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lms_models = {\n",
    "    # \"qwen3-1.7b\": \"qwen3-1.7b\",\n",
    "    \"exaone-3.5-2.4b-instruct\": \"exaone-3.5-2.4b-instruct\",\n",
    "    \"kakaocorp.kanana-nano-2.1b-instruct\": \"kakaocorp.kanana-nano-2.1b-instruct\",\n",
    "    \"hyperclovax-seed-text-instruct-1.5b-hf-i1\": \"hyperclovax-seed-text-instruct-1.5b-hf-i1\",\n",
    "    \"qwen3-4b\": \"qwen3-4b\",\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "ollama_models = {\n",
    "    \"exaone\": \"exaone3.5\",\n",
    "    \"clova\": \"yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B\",\n",
    "    \"kanana\": \"huihui_ai/kanana-nano-abliterated\",  # 2.1b\n",
    "    \"llama\": \"benedict/linkbricks-llama3.1-korean:8b\",  # 8b quantized\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.]\", \"_\", filename)[:20] + \".json\"\n",
    "\n",
    "\n",
    "for model_name, model_signature in lms_models.items():\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    llm = create_llm_lms(model_signature)\n",
    "    # llm = create_llm_ollama(signature)\n",
    "    retriever = create_retriever(vectorstore)\n",
    "    rag_chain = create_rag_chain(llm, retriever)\n",
    "    result_list = []\n",
    "    question_list = [\n",
    "        \"지하철 역에 대해 알려줘\",\n",
    "        \"서면역의 주변 명소를 알려줘\",\n",
    "        \"어른1 아이 2명의 1구간 요금은?\",\n",
    "    ]\n",
    "    for question in tqdm(question_list, desc=f\"Evaluating {model_name}\"):\n",
    "        result = rag_chain.invoke(question)\n",
    "        result_list.append({\"question\": question, \"answer\": result})\n",
    "    with open(sanitize_filename(f\"result_{model_name}.json\"), \"w\") as f:\n",
    "        json.dump(result_list, f, ensure_ascii=False)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['47e962c4-5edb-4455-9c9f-7c4cf2ba0f62',\n",
       "  'ecc4ed95-7af1-470c-ab99-b58a1f1c1957'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 질문 목록\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG_EVAL_DATASET_DEMO\"\n",
    "\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_dataset(client, dataset_name, description=None):\n",
    "    for dataset in client.list_datasets():\n",
    "        if dataset.name == dataset_name:\n",
    "            return dataset\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_dataset(client, dataset_name)\n",
    "\n",
    "new_questions = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 무엇인가요?\",\n",
    "    \"구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?\",\n",
    "]\n",
    "\n",
    "# 새로운 답변 목록\n",
    "new_answers = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 테디노트 입니다.\",\n",
    "    \"사실이 아닙니다. 구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자하기로 했습니다.\",\n",
    "]\n",
    "\n",
    "# UI에서 업데이트된 버전 확인\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
