{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "HUMETRO_LOCAL_RAG_EVAL_TEST\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"HUMETRO_LOCAL_RAG_EVAL_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Any\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# 2. 임베딩 모델 생성\n",
    "def create_embeddings(model_name: str = \"text-embedding-3-small\") -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_directory: str, embeddings: Any) -> Chroma:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_documents\",\n",
    "    )\n",
    "    print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# 3. 검색기(retriever) 생성 함수\n",
    "def create_retriever(vectorstore: Chroma, k: int = 4) -> Any:\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "# 4. LLM 모델 생성 함수\n",
    "def create_llm_ollama(model_name: str, temperature: float = 0.1) -> Any:\n",
    "    if \"ollama\" in model_name:\n",
    "        # Ollama 모델 확인 및 없으면 다운로드\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"ps\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            running_servers = result.stdout.strip()\n",
    "\n",
    "            # 2. 실행 중인 모델이 있으면 종료\n",
    "            if running_servers:\n",
    "                print(\"현재 실행 중인 모델을 종료합니다...\")\n",
    "                for line in running_servers.split(\"\\n\"):\n",
    "                    if line and not line.startswith(\"NAME\"):  # 헤더 행 제외\n",
    "                        running_model = line.split()[0]\n",
    "                        # 실행 중인 모델이 요청된 모델과 다른 경우에만 종료\n",
    "                        if running_model != model_name:\n",
    "                            print(f\"모델 {running_model}을 종료합니다...\")\n",
    "                            subprocess.run(\n",
    "                                [\"ollama\", \"stop\", running_model],\n",
    "                                check=True,\n",
    "                                capture_output=True,\n",
    "                            )\n",
    "            # 사용 가능한 모델 목록 확인\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"list\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            available_models = result.stdout.lower()\n",
    "\n",
    "            # 모델 이름에서 태그 분리\n",
    "            if \":\" in model_name:\n",
    "                base_model = model_name.split(\":\")[0]\n",
    "            else:\n",
    "                base_model = model_name\n",
    "\n",
    "            # 모델이 없으면 다운로드\n",
    "            if base_model not in available_models:\n",
    "                print(f\"모델 {model_name}을 다운로드합니다...\")\n",
    "                subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "                print(f\"모델 {model_name} 다운로드 완료\")\n",
    "            else:\n",
    "                print(f\"모델 {model_name}이 이미 존재합니다.\")\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Ollama 명령 실행 중 오류 발생: {e}\")\n",
    "            print(\"Ollama가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama 모델 준비 중 오류 발생: {e}\")\n",
    "\n",
    "        # Ollama 모델 연결\n",
    "        return Ollama(model=model_name, temperature=temperature)\n",
    "    else:\n",
    "        # OpenAI 모델 사용\n",
    "        return ChatOpenAI(model=model_name, temperature=temperature)\n",
    "\n",
    "\n",
    "def create_llm_lms(model_signature: str, temperature: float = 0.1) -> Any:\n",
    "    if \"gpt\" in model_signature:\n",
    "        return ChatOpenAI(model=model_signature, temperature=temperature)\n",
    "    try:\n",
    "        subprocess.run([\"lms\", \"unload\", \"-a\"])\n",
    "        print(\"모든 lms 언로드 완료\")\n",
    "        subprocess.run([\"lms\", \"load\", model_signature])\n",
    "        print(f\"lms 모델 {model_signature} 로드 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"lms 명령 실행 중 오류 발생: {e}\")\n",
    "        print(\"lms가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "    return ChatOpenAI(base_url=\"http://localhost:1234/v1\", model=model_signature)\n",
    "\n",
    "\n",
    "# 5. RAG 체인 생성 함수\n",
    "def create_rag_chain(llm: Any, retriever: Any) -> Any:\n",
    "    # 한국의 역무환경을 고려한 RAG 프롬프트 템플릿\n",
    "    template = \"\"\"\n",
    "당신은 한국의 도시철도 역무 지식 도우미입니다.\n",
    "주어진 질문에 대해 제공된 문맥 정보를 기반으로 정확하고 도움이 되는 답변을 제공하세요.\n",
    "문맥에 없는 내용은 답변하지 마세요. 모르는 경우 솔직히 모른다고 말하세요.\n",
    "\n",
    "문맥 정보:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # LCEL을 사용한 RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터스토어 로드 완료. 문서 수: 150\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = create_embeddings()\n",
    "vectorstore = load_vectorstore(persist_directory=\"vectorstore\", embeddings=embeddings)\n",
    "if len(vectorstore.similarity_search(\"서울역 주변 명소\")) == 0:\n",
    "    raise ValueError(\"vectorstore is empty\")\n",
    "if len(vectorstore.get()[\"ids\"]) == 0:\n",
    "    raise ValueError(\"vectorstore is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lms_models = {\n",
    "    # \"qwen3-1.7b\": \"qwen3-1.7b\",\n",
    "    \"exaone-3.5-2.4b-instruct\": \"exaone-3.5-2.4b-instruct\",\n",
    "    # \"kakaocorp.kanana-nano-2.1b-instruct\": \"kakaocorp.kanana-nano-2.1b-instruct\",\n",
    "    # \"hyperclovax-seed-text-instruct-1.5b-hf-i1\": \"hyperclovax-seed-text-instruct-1.5b-hf-i1\",\n",
    "    # \"qwen3-4b\": \"qwen3-4b\",\n",
    "    # \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "ollama_models = {\n",
    "    \"exaone\": \"exaone3.5\",\n",
    "    \"clova\": \"yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B\",\n",
    "    \"kanana\": \"huihui_ai/kanana-nano-abliterated\",  # 2.1b\n",
    "    \"llama\": \"benedict/linkbricks-llama3.1-korean:8b\",  # 8b quantized\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.]\", \"_\", filename)[:20] + \".json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['도시철도법시행령에 따른 운임조정 절차는 어떻게 진행되나요?', '부산지하철의 운임 조정과 관련하여 시·도지사의 역할과 운임조정위원회의 구성 및 기능에 대해 자세히 알려주세요.', '부산 지하철의 각 노선에 걸친 엘리베이터 현황은 어떻게 되며, 특히 분포와 운영 상태에 대해 알려주세요?', '서면역에 수유실이 있나요?', '안녕하세요, 벡스코가 부산 지하철 시스템에서 에스컬레이터 유지보수를 위한 전용 장소 중 하나인지 알려주실 수 있나요?', '부산 도시철도 역세권 주차장 중 부산교통공사가 관리하는 주차 공간에 대한 상세 정보를 알려주세요. 주차 면수와 연락처를 포함해 주세요.', '안녕하세요, 부산시설공단이 지하철에서 관리하는 장소는 어디인가요?', '현재 구역 분류에 따른 수영 지역의 부산시 주차 요금', '동매역 1호선에서 지정된 만남의 장소 수에 대한 정보를 제공해 주실 수 있나요?', '안녕하세요, 수영역에 2호선과 3호선의 만남의 장소가 각각 몇 개 있는지 알려주실 수 있나요? 숫자와 관련된 것들이 좀 헷갈려서 부산에서 만남을 계획하는 데 필요하거든요. 이 정보로 도와주시면 감사하겠습니다.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 질문 데이터 불러오기\n",
    "question_data = pd.read_csv(\"./translated_output.csv\")\n",
    "questions = list(question_data[\"user_input\"])\n",
    "print(questions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_signature in lms_models.items():\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    llm = create_llm_lms(model_signature)\n",
    "    retriever = create_retriever(vectorstore)\n",
    "    rag_chain = create_rag_chain(llm, retriever)\n",
    "    filename = sanitize_filename(f\"result_{model_name}.json\")\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            result_list = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        result_list = []\n",
    "\n",
    "    # 이미 처리된 질문 제외\n",
    "    processed_question = [i[\"question\"] for i in result_list]\n",
    "    questions = [i for i in questions if i not in processed_question]\n",
    "\n",
    "    for question in tqdm(questions, desc=f\"Evaluating {model_name}\"):\n",
    "        result = rag_chain.invoke(question)\n",
    "        result_list.append({\"question\": question, \"answer\": result})\n",
    "        if len(result_list) % 10 == 0:  # 10개마다 저장하기\n",
    "            print(f\"checkpoint, saved {len(result_list)}\")\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(result_list, f, ensure_ascii=False)\n",
    "    with open(filename, \"w\") as f:  # 최종 결과 저장하기\n",
    "        json.dump(result_list, f, ensure_ascii=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANG SMITH CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['47e962c4-5edb-4455-9c9f-7c4cf2ba0f62',\n",
       "  'ecc4ed95-7af1-470c-ab99-b58a1f1c1957'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 질문 목록\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG_EVAL_DATASET_DEMO\"\n",
    "\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_dataset(client, dataset_name, description=None):\n",
    "    for dataset in client.list_datasets():\n",
    "        if dataset.name == dataset_name:\n",
    "            return dataset\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_dataset(client, dataset_name)\n",
    "\n",
    "new_questions = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 무엇인가요?\",\n",
    "    \"구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?\",\n",
    "]\n",
    "\n",
    "# 새로운 답변 목록\n",
    "new_answers = [\n",
    "    \"삼성전자가 만든 생성형 AI의 이름은 테디노트 입니다.\",\n",
    "    \"사실이 아닙니다. 구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자하기로 했습니다.\",\n",
    "]\n",
    "\n",
    "# UI에서 업데이트된 버전 확인\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
