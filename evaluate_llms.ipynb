{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "HUMETRO_LOCAL_RAG_EVAL_TEST\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"HUMETRO_LOCAL_RAG_EVAL_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Any\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# 2. 임베딩 모델 생성\n",
    "def create_embeddings(model_name: str = \"text-embedding-3-small\") -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_directory: str, embeddings: Any) -> Chroma:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_documents\",\n",
    "    )\n",
    "    print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# 3. 검색기(retriever) 생성 함수\n",
    "def create_retriever(vectorstore: Chroma, k: int = 4) -> Any:\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "# 4. LLM 모델 생성 함수\n",
    "def create_llm(model_name: str, temperature: float = 0.1) -> Any:\n",
    "    if \"gpt\" not in model_name:\n",
    "        # Ollama 모델 확인 및 없으면 다운로드\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"ps\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            running_servers = result.stdout.strip()\n",
    "\n",
    "            # 2. 실행 중인 모델이 있으면 종료\n",
    "            if running_servers:\n",
    "                print(\"현재 실행 중인 모델을 종료합니다...\")\n",
    "                for line in running_servers.split(\"\\n\"):\n",
    "                    if line and not line.startswith(\"NAME\"):  # 헤더 행 제외\n",
    "                        running_model = line.split()[0]\n",
    "                        # 실행 중인 모델이 요청된 모델과 다른 경우에만 종료\n",
    "                        if running_model != model_name:\n",
    "                            print(f\"모델 {running_model}을 종료합니다...\")\n",
    "                            subprocess.run(\n",
    "                                [\"ollama\", \"stop\", running_model],\n",
    "                                check=True,\n",
    "                                capture_output=True,\n",
    "                            )\n",
    "            # 사용 가능한 모델 목록 확인\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"list\"], capture_output=True, text=True, check=True\n",
    "            )\n",
    "            available_models = result.stdout.lower()\n",
    "\n",
    "            # 모델 이름에서 태그 분리\n",
    "            if \":\" in model_name:\n",
    "                base_model = model_name.split(\":\")[0]\n",
    "            else:\n",
    "                base_model = model_name\n",
    "\n",
    "            # 모델이 없으면 다운로드\n",
    "            if base_model not in available_models:\n",
    "                print(f\"모델 {model_name}을 다운로드합니다...\")\n",
    "                subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "                print(f\"모델 {model_name} 다운로드 완료\")\n",
    "            else:\n",
    "                print(f\"모델 {model_name}이 이미 존재합니다.\")\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Ollama 명령 실행 중 오류 발생: {e}\")\n",
    "            print(\"Ollama가 설치되어 있고 실행 중인지 확인하세요.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama 모델 준비 중 오류 발생: {e}\")\n",
    "\n",
    "        # Ollama 모델 연결\n",
    "        return Ollama(model=model_name, temperature=temperature)\n",
    "    else:\n",
    "        # OpenAI 모델 사용\n",
    "        return ChatOpenAI(model=model_name, temperature=temperature)\n",
    "\n",
    "\n",
    "# 5. RAG 체인 생성 함수\n",
    "def create_rag_chain(llm: Any, retriever: Any) -> Any:\n",
    "    # 한국의 역무환경을 고려한 RAG 프롬프트 템플릿\n",
    "    template = \"\"\"\n",
    "당신은 한국의 도시철도 역무 지식 도우미입니다.\n",
    "주어진 질문에 대해 제공된 문맥 정보를 기반으로 정확하고 도움이 되는 답변을 제공하세요.\n",
    "문맥에 없는 내용은 답변하지 마세요. 모르는 경우 솔직히 모른다고 말하세요.\n",
    "\n",
    "문맥 정보:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # LCEL을 사용한 RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터스토어 로드 완료. 문서 수: 150\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = create_embeddings()\n",
    "vectorstore = load_vectorstore(persist_directory=\"vectorstore\", embeddings=embeddings)\n",
    "if len(vectorstore.similarity_search(\"서울역 주변 명소\")) == 0:\n",
    "    raise ValueError(\"vectorstore is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exaone...\n",
      "현재 실행 중인 모델을 종료합니다...\n",
      "모델 exaone3.5이 이미 존재합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating exaone: 100%|██████████| 3/3 [00:59<00:00, 19.89s/it]\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading clova...\n",
      "현재 실행 중인 모델을 종료합니다...\n",
      "모델 exaone3.5:latest을 종료합니다...\n",
      "모델 yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B을 다운로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 63fcaa2d012b: 100% ▕██████████████████▏   29 B                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 32ba8b50b4ce: 100% ▕██████████████████▏  635 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B 다운로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clova: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading kanana...\n",
      "현재 실행 중인 모델을 종료합니다...\n",
      "모델 yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B:latest을 종료합니다...\n",
      "모델 huihui_ai/kanana-nano-abliterated이 이미 존재합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating kanana: 100%|██████████| 3/3 [00:25<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading llama...\n",
      "현재 실행 중인 모델을 종료합니다...\n",
      "모델 huihui_ai/kanana-nano-abliterated:latest을 종료합니다...\n",
      "모델 benedict/linkbricks-llama3.1-korean:8b이 이미 존재합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating llama: 100%|██████████| 3/3 [01:00<00:00, 20.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt-4o-mini: 100%|██████████| 3/3 [00:08<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"exaone\": \"exaone3.5\",\n",
    "    \"clova\": \"yoonyoung/HyperCLOVAX-SEED-Vision-Instruct-3B\",\n",
    "    \"kanana\": \"huihui_ai/kanana-nano-abliterated\",  # 2.1b\n",
    "    \"llama\": \"benedict/linkbricks-llama3.1-korean:8b\",  # 8b quantized\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r\"[^a-zA-Z0-9_.]\", \"_\", filename)[:20] + \".json\"\n",
    "\n",
    "\n",
    "for model_name, signature in models.items():\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    llm = create_llm(signature)\n",
    "    retriever = create_retriever(vectorstore)\n",
    "    rag_chain = create_rag_chain(llm, retriever)\n",
    "    result_list = []\n",
    "    question_list = [\n",
    "        \"지하철 역에 대해 알려줘\",\n",
    "        \"서면역의 주변 명소를 알려줘\",\n",
    "        \"어른1 아이 2명의 1구간 요금은?\",\n",
    "    ]\n",
    "    for question in tqdm(question_list, desc=f\"Evaluating {model_name}\"):\n",
    "        result = rag_chain.invoke(question)\n",
    "        result_list.append({\"question\": question, \"answer\": result})\n",
    "    with open(sanitize_filename(f\"result_{model_name}.json\"), \"w\") as f:\n",
    "        json.dump(result_list, f, ensure_ascii=False)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
