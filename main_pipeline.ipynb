{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def load_documents(doc_dir: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    특정 디렉토리에서 마크다운 문서를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        doc_dir: 마크다운 문서가 있는 디렉토리 경로\n",
    "\n",
    "    Returns:\n",
    "        문서 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 마크다운 로더 생성\n",
    "        loader = DirectoryLoader(\n",
    "            doc_dir,\n",
    "            # 현재폴더의 마크다운만 로드하고, 자식 폴더는 로드하지 않음\n",
    "            glob=\"*.md\",\n",
    "            loader_cls=UnstructuredMarkdownLoader,\n",
    "            show_progress=True,\n",
    "            recursive=False,  # 하위 디렉토리도 검색\n",
    "        )\n",
    "\n",
    "        # 문서 로드\n",
    "        documents = loader.load()\n",
    "\n",
    "        print(f\"로드된 문서 수: {len(documents)}\")\n",
    "\n",
    "        # 파일 이름을 metadata에 추가\n",
    "        for doc in documents:\n",
    "            if \"source\" in doc.metadata:\n",
    "                doc.metadata[\"filename\"] = os.path.basename(doc.metadata[\"source\"])\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"문서 로드 중 오류 발생: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 100,\n",
    "    include_filename_in_content: bool = False,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    문서를 청크로 분할하면서 파일 이름 정보를 유지합니다.\n",
    "\n",
    "    Args:\n",
    "        documents: 분할할 문서 리스트\n",
    "        chunk_size: 각 청크의 크기(토큰 수)\n",
    "        chunk_overlap: 인접한 청크 간의 겹치는 토큰 수\n",
    "        include_filename_in_content: 파일 이름을 본문에도 포함할지 여부\n",
    "\n",
    "    Returns:\n",
    "        분할된 문서 청크 리스트\n",
    "    \"\"\"\n",
    "    # 파일 이름을 본문에 포함하는 경우, 새 문서 리스트 생성\n",
    "    if include_filename_in_content:\n",
    "        preprocessed_docs = []\n",
    "        for doc in documents:\n",
    "            filename = doc.metadata.get(\"filename\", \"Unknown\")\n",
    "            new_content = f\"파일: {filename}\\n\\n{doc.page_content}\"\n",
    "\n",
    "            # 새 문서 생성 (메타데이터는 유지)\n",
    "            new_doc = Document(page_content=new_content, metadata=doc.metadata)\n",
    "            preprocessed_docs.append(new_doc)\n",
    "\n",
    "        # 처리할 문서를 전처리된 문서로 교체\n",
    "        documents = preprocessed_docs\n",
    "\n",
    "    # 토큰 기반 분할기 생성\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        # length_function=len,\n",
    "    )\n",
    "\n",
    "    # 문서 분할\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"원본 문서 수: {len(documents)}, 분할 후 청크 수: {len(splits)}\")\n",
    "\n",
    "    # 청크 길이 통계\n",
    "    lengths = [len(doc.page_content) for doc in splits]\n",
    "    if lengths:\n",
    "        print(\n",
    "            f\"청크 길이 - 평균: {sum(lengths) / len(lengths):.1f}, 최소: {min(lengths)}, 최대: {max(lengths)}\"\n",
    "        )\n",
    "\n",
    "    # 메타데이터 확인 (샘플)\n",
    "    if splits:\n",
    "        print(f\"첫 번째 청크 메타데이터 샘플: {splits[0].metadata}\")\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def save_splits_as_markdown(splits: List[Document], output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    분할된 문서 청크를 마크다운 파일로 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        splits: 분할된 문서 청크 리스트\n",
    "        output_dir: 저장할 디렉토리 경로 (기존 디렉토리 내에 새 하위 디렉토리가 생성됨)\n",
    "\n",
    "    Returns:\n",
    "        생성된 디렉토리 경로\n",
    "    \"\"\"\n",
    "    # 타임스탬프를 사용하여 고유한 디렉토리 이름 생성\n",
    "    from datetime import datetime\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    splits_dir = os.path.join(output_dir, f\"splits_{timestamp}\")\n",
    "\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "    # 파일 이름에 사용할 수 없는 문자 패턴\n",
    "    invalid_chars_pattern = re.compile(r'[\\\\/*?:\"<>|]')\n",
    "\n",
    "    # 각 청크를 별도의 마크다운 파일로 저장\n",
    "    for i, doc in enumerate(splits):\n",
    "        # 메타데이터에서 원본 파일 이름 가져오기\n",
    "        original_filename = doc.metadata.get(\"filename\", \"unknown\")\n",
    "\n",
    "        # 파일 확장자 제거\n",
    "        original_filename = os.path.splitext(original_filename)[0]\n",
    "\n",
    "        # 파일 이름에 사용할 수 없는 문자 제거\n",
    "        safe_filename = invalid_chars_pattern.sub(\"_\", original_filename)\n",
    "\n",
    "        # 저장할 파일 경로 생성\n",
    "        output_filename = f\"{safe_filename}_chunk_{i + 1:03d}.md\"\n",
    "        output_path = os.path.join(splits_dir, output_filename)\n",
    "\n",
    "        # 마크다운 내용 생성\n",
    "        markdown_content = f\"# 청크 {i + 1} - 원본 파일: {original_filename}\\n\\n\"\n",
    "\n",
    "        # 메타데이터 섹션 추가\n",
    "        markdown_content += \"## 메타데이터\\n\\n\"\n",
    "        markdown_content += \"```\\n\"\n",
    "        for key, value in doc.metadata.items():\n",
    "            markdown_content += f\"{key}: {value}\\n\"\n",
    "        markdown_content += \"```\\n\\n\"\n",
    "\n",
    "        # 본문 내용 추가\n",
    "        markdown_content += \"## 본문 내용\\n\\n\"\n",
    "        markdown_content += doc.page_content\n",
    "\n",
    "        # 파일 저장\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_content)\n",
    "\n",
    "    print(f\"총 {len(splits)}개의 청크가 {splits_dir} 디렉토리에 저장되었습니다.\")\n",
    "    return splits_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:05<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 문서 수: 36\n",
      "원본 문서 수: 36, 분할 후 청크 수: 150\n",
      "청크 길이 - 평균: 352.3, 최소: 16, 최대: 1125\n",
      "첫 번째 청크 메타데이터 샘플: {'source': 'datasets/final_docs/도시철도 수송원가 및 운임 결정 과정_processed.md', 'filename': '도시철도 수송원가 및 운임 결정 과정_processed.md'}\n",
      "총 150개의 청크가 datasets/final_docs/splits_20250514_094625 디렉토리에 저장되었습니다.\n",
      "청크 확인 경로: datasets/final_docs/splits_20250514_094625\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 문서 로드 및 분할\n",
    "doc_dir = Path(\".\") / \"datasets\" / \"final_docs\"\n",
    "documents = load_documents(doc_dir)\n",
    "splits = split_documents(documents)\n",
    "\n",
    "# 분할된 청크를 마크다운 파일로 저장\n",
    "output_path = save_splits_as_markdown(splits, Path(\".\") / \"datasets\" / \"final_docs\")\n",
    "print(f\"청크 확인 경로: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model_name=\"text-embedding-3-small\"):\n",
    "    return OpenAIEmbeddings(model=model_name, dimensions=1536)\n",
    "\n",
    "\n",
    "def create_vectorstore(\n",
    "    splits, embeddings, persist_dir, collection_name=\"rag_documents\"\n",
    "):\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "\n",
    "        print(f\"기존 벡터스토어 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "        if splits:\n",
    "            vectorstore.add_documents(splits)\n",
    "            vectorstore.persist()\n",
    "            print(\n",
    "                f\"벡터스토어 업데이트 완료. 총 문서 수: {vectorstore._collection.count()}\"\n",
    "            )\n",
    "    else:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_dir,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        vectorstore.persist()\n",
    "        print(f\"벡터스토어 생성 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def load_vectorstore(persist_dir, embeddings, collection_name=\"rag_documents\"):\n",
    "    if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(f\"벡터스토어 로드 완료. 문서 수: {vectorstore._collection.count()}\")\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(f\"벡터스토어를 찾을 수 없습니다: {persist_dir}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터스토어 생성 완료. 문서 수: 150\n",
      "벡터스토어 로드 완료. 문서 수: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/zz8kzx7j04q6lvkcyzwjznwh0000gn/T/ipykernel_93376/1704130537.py:32: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n",
      "/var/folders/lg/zz8kzx7j04q6lvkcyzwjznwh0000gn/T/ipykernel_93376/1704130537.py:40: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 생성\n",
    "embeddings = create_embeddings()\n",
    "\n",
    "# 벡터스토어 생성 및 저장\n",
    "persist_dir = \"vectorstore\"\n",
    "vectorstore = create_vectorstore(\n",
    "    splits=splits, embeddings=embeddings, persist_dir=persist_dir\n",
    ")\n",
    "\n",
    "\n",
    "# # 나중에 벡터스토어 불러오기\n",
    "loaded_vectorstore = load_vectorstore(persist_dir, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "from ragas.testset.synthesizers.multi_hop.specific import (\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_qa_dataset(\n",
    "    splits, output_path=\"datasets/synthetic_qa_dataset.csv\", num_questions=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a synthetic QA dataset using RAGAS.\n",
    "\n",
    "    Args:\n",
    "        splits: List of Document objects with the content to generate questions from\n",
    "        output_path: Path to save the CSV dataset\n",
    "        num_questions: Number of questions to generate\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with the generated dataset\n",
    "    \"\"\"\n",
    "    # Initialize LLM and embeddings wrappers\n",
    "    llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "    # llm = LangchainLLMWrapper(ChatXAI(model=\"grok-3-beta\"))\n",
    "    embeddings = LangchainEmbeddingsWrapper(\n",
    "        OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    )\n",
    "\n",
    "    # Create TestsetGenerator (현재 API는 llm과 embedding_model만 받음)\n",
    "    generator = TestsetGenerator(llm=llm, embedding_model=embeddings)\n",
    "\n",
    "    distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=llm), 0.7),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=llm), 0.3),\n",
    "    ]\n",
    "\n",
    "    for query, _ in distribution:\n",
    "        prompts = await query.adapt_prompts(\n",
    "            \"## 매우 중요: **한국어로만 질문과 답변을 생성**, Question and Answer MUST be in KOREAN\",\n",
    "            llm=llm,\n",
    "        )\n",
    "        query.set_prompts(**prompts)\n",
    "\n",
    "    # Generate testset\n",
    "    print(f\"Generating {num_questions} QA pairs...\")\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=splits,\n",
    "        testset_size=num_questions,\n",
    "        query_distribution=distribution,\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    test_df = testset.to_pandas()\n",
    "\n",
    "    # Save to CSV\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    test_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"QA dataset created with {len(test_df)} question-answer pairs\")\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
