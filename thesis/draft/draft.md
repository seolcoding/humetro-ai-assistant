# 온프레미스 오픈소스 기반 Graph RAG 시스템의 공공부문 적용 연구: 다산콜센터 사례를 중심으로 (분량 조절_한글파일 원본)

### 1.1 연구 배경 및 필요성
#### 1.1.1 공공부문 AI 도입의 가속화와 주권 딜레마

AI 기술의 공공 행정 도입 여부에 대한 사회적 합의는 이미 완료되었다. 2024년 기준 전 세계 조직의 78%가 AI를 활용 중이며, 대한민국 공공기관의 60.6%가 이미 AI를 도입했다. AI 관련 공공 조달액은 10년간 약 4.7배 증가하며 공공부문 AI 전환이 가속화되고 있다 (소프트웨어정책연구소, 2025; Stanford University, 2025).

이러한 변화는 정부의 전략적 정책 추진에 의해 가속화되고 있다. 행정안전부는 2024년 '생성형 AI 공공서비스 적용 가이드라인'을 발표하며 전국 지자체와 공공기관의 챗봇, 민원 자동응답 등 생성형 AI 도입을 장려하고 있다 (행정안전부, 2024). 이러한 흐름 속에서 서울시 120다산콜센터는 2025년까지 'AI 기반 스마트 상담센터' 구축을 목표로 하는 '다산콜 2.0' 이니셔티브를 추진 중이며, 이미 STT 시스템과 '서울톡' 챗봇을 도입하여 전체 상담의 16.7%를 처리하는 등 가시적 성과를 거두고 있다 (서울시, 2024).

그러나 이 과정에서 공공기관들은 '주권 딜레마'에 직면한다. 클라우드 기술의 신속성과 유연성을 수용하는 동시에, 비용 통제, 데이터 주권, 보안이라는 핵심 가치를 지켜야 하는 문제이다. 이에 대한 대안으로 한국은행, 국회도서관, 한국전력 등 국가 핵심 기관들은 데이터 보안과 기술 자립을 위해 온프레미스 방식을 적극 채택하고 있다 (한국IDC, 2024). 이는 온프레미스 방식이 공공 부문의 주권 딜레마를 해결할 핵심 전략으로 부상하고 있음을 시사한다.
#### 1.1.2 클라우드 API 의존의 구조적 문제

클라우드 기반 AI 서비스는 공공 부문이 장기적으로 채택하기 어려운 세 가지 구조적 문제를 내포한다.
첫째, 경제적 지속 불가능성이다. 상용 API는 종량제 모델을 채택하여 초기 비용은 낮지만, 하루 수만 건을 처리하는 대용량 서비스의 경우 사용량 증가에 따라 운영 비용이 예측 불가능하게 급증한다 (OpenAI, 2025; Moesif, 2025).

둘째, 데이터 주권 침해 위험이다. 민감한 시민 데이터를 외부 클라우드에 저장할 경우, 미국 CLOUD Act는 데이터 저장 위치와 무관하게 미국 기업이 통제하는 데이터에 자국 법 집행 기관의 접근을 허용한다 (Wire, 2025). 이는 시민의 민감 정보를 외국 사법 관할권에 두는 잠재적 위험이다.

셋째, 보안 취약성 문제이다. 외부 API를 호출하는 AI 에이전트는 환경 변수 조작이나 악의적 데이터 주입 공격에 취약하다 (Li et al., 2025; Kumar et al., 2025). 내부 작동을 감사할 수 없는 상용 API는 데이터 유출의 통로가 될 수 있으며, 이는 공공 서비스 신뢰를 훼손하는 심각한 보안 부채로 작용한다.

### **1.2 연구 목적**

#### **1.2.1 주 목적: 온프레미스 기반 RAG 스택의 공공 도메인 실용성 실증**

본 연구의 주 목적은 온프레미스 환경(NVIDIA RTX 3090Ti 24GB VRAM)에서 SOTA 모델(GPT-5)로 구축한 고품질 지식 그래프와 오픈소스 LLM을 결합한 하이브리드 RAG 시스템이 비용 효율적 상용 클라우드 API(GPT-4.1-mini, Gemini 2.5 Flash)를 활용한 Naive RAG 시스템에 근접하거나 동등한 성능을 달성할 수 있음을 실증하는 데 있다. 이는 최첨단 AI 성능 확보가 해외 빅테크 기업에 대한 기술적·재정적 의존을 통해서만 가능하다는 통념을 깨고, 기술 및 데이터 주권 확보와 고성능 AI 서비스 도입이 양립 가능함을 증명하는 것을 목표로 한다.

핵심 전략은 "복잡한 일은 최고 모델로, 반복적 일은 효율적 모델로"라는 원칙을 구현하는 것이다. GPT-5를 활용하여 민원 데이터로부터 도메인 특화 지식 그래프를 일회성으로 구축한 후, 이를 기반으로 오픈소스 LLM을 활용한 Graph RAG 시스템을 구현한다. 이를 통해 본 연구는 공공기관이 데이터 주권과 비용 통제권을 유지하면서도 시민들에게 최상의 AI 기반 서비스를 제공할 수 있는 기술적 청사진을 제시하고자 한다.

#### **1.2.2 세부 목표**

1. **SOTA 모델 기반 지식 그래프 구축**: GPT-5를 활용하여 원시 민원 데이터로부터 도메인 특화 지식 그래프를 자동 구축한다. 일회성 작업으로 상용 API 비용($200-500)이 허용 가능하며, 구축된 고품질 KG는 RAG 시스템의 영구적 자산이 된다. 오픈소스 LLM과의 KG 품질 차이를 정량화하여 SOTA 모델의 필요성을 입증한다.

2. **계층적 RAG 비교를 통한 KG 기여도 정량화**: 4단계 계층적 RAG 모델을 비교한다—(1) Baseline (RAG 없음), (2) Naive RAG (벡터 검색), (3) Structured RAG (메타데이터 필터링), (4) Graph RAG (다중 홉 추론). 각 단계의 성능 향상을 측정하여 고품질 KG가 RAG 성능에 미치는 결정적 기여를 입증한다.

3. **한국어 공공 민원 도메인 과제 검증**: 한국어 공공 민원의 실무적 과제를 검증한다—(1) 언어 스타일 이질성 (공문서체와 구어체 혼재), (2) 도메인 특화 용어 처리, (3) 불완전 정보 대응. 4개 오픈소스 LLM(EXAONE-3.5 7.8B/32B, Qwen 3-8B, Gemma 3-12B, GPT-OSS-20B)의 양자화 버전을 비교하여 도메인 특화 과제에 대한 모델별 강건성을 검증한다.

4. **하이브리드 전략의 TCO 우위 입증**: RTX 3090 기반 온프레미스는 초기 투자(HW $2,000 + KG 구축 $200-500)를 요구하지만, 5년 TCO는 $8,200로 순수 상용 API 대비 최대 36% 절감된다. KG 구축 비용은 전체 TCO의 3-6%에 불과하여 일회성 고품질 투자의 정당성을 입증한다.

## **1.3 연구 대상, 내용 및 범위**

### **1.3.1 연구 대상: 120 다산콜센터**

본 연구는 서울시 **120 다산콜센터**를 연구 대상 기관으로 선정하였다. 다산콜센터는 서울시민의 생활 민원 상담을 전담하는 핵심 공공서비스 플랫폼으로, 2025년 현재 **65명 이상의 상담원**이 **일평균 4,000건 이상**의 민원을 처리하고 있다^[서울시 다산콜센터 운영 현황, 2025].

상담 내용은 행정 절차, 복지 제도, 생활 불편 신고 등 매우 다양하며, 반복적인 질의가 높은 비중을 차지하여 AI 기반 자동화 시스템 도입을 통한 상담 효율화 필요성이 지속적으로 제기되어 왔다. 그러나 민원 데이터의 특수성(구어체 표현, 감정 표현, 복합 질의)과 개인정보 보호 요구사항으로 인해, **온프레미스 환경**에서 구동 가능한 고성능 AI 시스템이 필수적이다.

### **1.3.2 비교 실험 설계**

본 연구는 **4개 오픈소스 LLM** × **4개 RAG 방식**의 조합으로 총 **16개 시스템**을 구축하고, 이를 상용 API 기준 모델과 비교 평가한다:

- **오픈소스 LLM**: Gemma 3 12B, Qwen 3 8B, EXAONE 7.8B, gpt-oss-20b (필요시 24GB VRAM에 맞게 quantized 모델 활용)
- **RAG 방식**: pure LLM, Naive RAG (벡터 검색), Advanced RAG(hybrid search), Graph RAG (지식 그래프 기반)
- **베이스라인**: GPT-4o-mini + Naive RAG (상용 API)

### **1.3.3 데이터 구성**

- **지식 베이스: 서울시 공공데이터 포털의 "다산콜센터 FAQ API"를 통해 수집한 3,000개 이상의 최신 업무 매뉴얼 및 상담 가이드**
- **평가 데이터: AI Hub "다산콜센터 Q&A 데이터셋"에서 추출한 10,000개 질의-응답 쌍을 Ground Truth로 활용**

### **1.3.4 실험 환경**

**온프레미스 환경은 NVIDIA RTX 3090Ti (24GB VRAM) 기반 워크스테이션(총 $2,000-2,500)으로 구성하며, Neo4j(그래프 DB), LangChain(오케스트레이션), vLLM(추론 엔진)을 활용한다. 평가는 RAGAS 프레임워크를 통한 정량 평가와 GPT-4o 기반 LLM-as-a-Judge 정성 평가를 병행하며, TCO(총소유비용) 및 ROI(투자수익률) 분석을 통해 비용 효율성을 검증한다.**

## **1.4 연구의 차별점과 기여도**

**첫째, Ablation Study를 통한 구성요소별 기여도 정량화**를 제공한다. 기존 Graph RAG 연구들은 전체적 성능 우위를 입증했으나^[Microsoft Graph RAG, 2024; LightRAG, 2024], 각 구성요소(메타데이터 필터링, 그래프 구조화, 다중 홉 추론)가 성능에 미치는 영향을 분리하여 측정하지 못했다. 본 연구는 **체계적 Ablation Study**를 수행하여 4단계 계층적 접근—(1) Baseline (RAG 없음), (2) Naive RAG (벡터 검색), (3) Structured RAG (메타데이터 필터링), (4) Graph RAG (그래프 추론)—각각의 한계 기여도를 정량적으로 분석한다. 이는 2024-2025년 급증한 Graph RAG 연구^[arXiv Graph RAG 서베이, 2025]가 개념 증명에서 정교한 엔지니어링으로 이동하는 시점에, 실무자들이 투자 대비 효과를 평가할 수 있는 방법론적 기준을 제시한다.

**둘째, 공공기관 도입 가능한 비용 효율적 솔루션**을 제시한다. 본 연구의 하이브리드 전략—SOTA 모델로 일회성 KG 구축($200-500) 후 오픈소스 LLM으로 추론—은 상용 API 의존 모델의 경제적 지속 불가능성을 해결한다. 온프레미스 방식은 초기 하드웨어 투자($2,000-2,500)를 요구하지만, 5년 TCO는 순수 클라우드 대비 최대 36% 절감되며, KG 구축 비용은 전체 TCO의 3-6%에 불과하여 일회성 고품질 투자의 정당성이 입증된다. 또한 RTX 4090급 상용 GPU에서 양자화된 7B-20B 모델로 실시간 서비스가 가능함을 실증하여, 지방자치단체 및 중소 공공기관도 현실적 예산으로 도입할 수 있는 복제 가능한 청사진을 제공한다.

**셋째, 고품질 지식 그래프 엔지니어링을 통한 온프레미스 SOTA 성능 달성 전략**을 실증한다. 본 연구의 핵심 가설은 "최첨단 AI 성능이 해외 빅테크 기업의 클라우드 API에 대한 영구적 의존을 통해서만 가능하다"는 통념에 도전하는 것이다. SOTA 모델(GPT-5)로 일회성 구축한 **고품질 도메인 특화 지식 그래프**와 정교한 **Graph RAG 엔지니어링**(다중 홉 추론, 관계 기반 가지치기)을 결합하면, 온프레미스 환경에서 구동되는 오픈소스 LLM이 비용 효율적 상용 API(GPT-4o-mini, Gemini 2.5 Flash)에 근접하거나 동등한 성능을 달성할 수 있음을 입증한다.

이는 단순히 "오픈소스 모델도 괜찮다"는 수준을 넘어, **지식의 구조화**와 **검색 메커니즘의 고도화**라는 엔지니어링적 혁신이 모델 자체의 성능 한계를 보완할 수 있다는 방법론적 통찰을 제공한다. 결과적으로 공공기관은 민감한 시민 데이터를 외부에 노출하지 않으면서도(데이터 주권), 예측 불가능한 종량제 비용 구조에서 벗어나(비용 통제), 해외 기업의 서비스 정책 변경에 영향받지 않는(공급업체 독립성) 지속 가능한 AI 서비스를 구축할 수 있다. 이는 디지털 주권 확보와 고품질 AI 서비스 제공이라는 두 목표가 상호 배타적이지 않음을 보여주는 실증적 증거가 된다.
# 2. 이론적 배경
## 2.1 인공지능과 자연어처리의 기술적 진화
### **2.1.1 인공지능의 발전 단계**

인공지능(Artificial Intelligence, AI)은 1956년 다트머스 회의(Dartmouth Conference)에서 존 매카시(John McCarthy)가 처음 제안한 이후, 약 70년간 여러 패러다임 전환을 거치며 발전해왔다. 이러한 발전 과정은 크게 네 단계로 구분할 수 있으며, 각 단계는 당시의 컴퓨팅 자원과 이론적 진보에 따라 서로 다른 접근 방식을 보였다.

**1) 규칙 기반 AI (1950-1980년대)**

초기 인공지능 연구는 인간 전문가의 지식을 명시적 규칙(explicit rules)으로 코드화하는 방식에 집중했다. 대표적인 사례로 MYCIN(1970년대)은 의료 진단을 위해 약 600개의 if-then 규칙을 활용한 전문가 시스템(expert system)이었다. 이러한 접근법은 특정 도메인에서는 우수한 성능을 보였으나, 규칙 작성에 막대한 인력이 필요하고 새로운 상황에 대한 일반화(generalization)가 어렵다는 근본적 한계를 가졌다. 특히, 자연어처리와 같이 예외가 많고 맥락 의존적인 문제에서는 규칙 기반 접근이 실용적이지 못했다.

**2) 통계 기반 머신러닝 (1990-2000년대)**

1990년대 들어 컴퓨팅 성능이 향상되고 데이터가 축적되면서, 데이터로부터 패턴을 학습하는 통계적 머신러닝이 주류로 부상했다. Support Vector Machine(SVM), Random Forest, Naive Bayes 등의 알고리즘이 문서 분류, 스팸 필터링, 감정 분석 등 다양한 자연어처리 작업에 적용되었다. 이 시기의 핵심 아이디어는 "명시적 규칙 대신 데이터가 말하게 하라"는 것이었다. 그러나 이러한 알고리즘들은 대부분 수작업으로 설계한 특징(hand-crafted features)에 의존했으며, 텍스트의 깊은 의미나 장거리 의존성을 포착하는 데는 한계가 있었다.

**3) 딥러닝 혁명 (2010년대)**

2012년 AlexNet이 ImageNet 경진대회에서 압도적 성능을 보이며, 딥러닝(Deep Learning)이 AI의 새로운 패러다임으로 자리잡았다. 자연어처리 분야에서도 Convolutional Neural Networks(CNN)와 Recurrent Neural Networks(RNN), 특히 Long Short-Term Memory(LSTM)가 기계번역, 문서 분류, 질의응답 등에서 기존 방법론을 크게 능가했다. 딥러닝의 핵심 강점은 특징 공학(feature engineering) 없이 원시 데이터로부터 계층적 표현(hierarchical representation)을 자동으로 학습한다는 점이다. 그러나 RNN 계열 모델은 순차적 처리(sequential processing) 특성상 병렬화가 어렵고, 문장이 길어질수록 초기 정보가 희석되는 문제(vanishing gradient problem)를 완전히 해결하지 못했다.

**4) 대규모 언어모델(LLM) 시대 (2017년~현재)**

2017년 Vaswani 등이 발표한 "Attention Is All You Need" 논문은 AI 역사의 전환점이 되었다. Transformer 아키텍처는 RNN의 순차 처리를 버리고 Self-Attention 메커니즘을 통해 문장 내 모든 단어 간의 관계를 병렬로 학습함으로써, 학습 속도와 성능을 동시에 향상시켰다. 이후 BERT(2018), GPT-2(2019), GPT-3(2020), 그리고 ChatGPT(2022)로 이어지는 대규모 언어모델의 급속한 발전은 자연어 이해와 생성 능력의 비약적 도약을 가져왔다. 특히, 수십억에서 수천억 개의 파라미터를 가진 모델이 방대한 텍스트 데이터로 사전학습(pre-training)된 후, 특정 작업에 미세조정(fine-tuning)되는 전이 학습(transfer learning) 패러다임이 표준이 되었다.

그러나 LLM의 급속한 발전과 함께 새로운 과제들도 드러났다. 학습 데이터의 시점 고정(knowledge cutoff), 사실이 아닌 정보를 생성하는 환각(hallucination), 그리고 도메인 특화 지식의 부족 등이 대표적이다. 이러한 한계를 극복하기 위해 외부 지식을 활용하는 Retrieval-Augmented Generation(RAG)과 같은 하이브리드 접근법이 주목받고 있으며, 본 연구에서 다루는 Graph RAG 역시 이러한 흐름의 연장선에 있다.

### **2.1.2 Transformer 아키텍처와 Self-Attention**

**1) RNN의 한계와 Transformer의 등장 배경**

2017년 이전까지 자연어처리의 주류는 Recurrent Neural Networks(RNN)와 그 변형인 LSTM, GRU였다. RNN은 문장을 순차적으로 처리하며 이전 시점의 정보를 hidden state에 축적하는 방식으로 작동한다. 그러나 이러한 순차 처리 특성은 두 가지 근본적 한계를 가진다.

첫째, **병렬화 불가능(non-parallelizable)** 문제이다. RNN은 t번째 단어를 처리하기 위해 반드시 t-1번째 계산이 완료되어야 하므로, GPU의 병렬 연산 능력을 충분히 활용할 수 없다. 이는 대규모 데이터셋 학습 시 치명적인 병목이 된다.

둘째, **장거리 의존성(long-range dependency)** 문제이다. 문장이 길어질수록 초기 단어의 정보가 점차 희석되며, LSTM의 cell state도 수십 개 이상의 단어를 거치면 정보 손실이 발생한다. 예를 들어 "The cat, which we found in the park last summer, was very friendly"에서 "cat"과 "was"의 관계를 학습하려면 중간의 긴 수식절을 거쳐야 한다.

Vaswani et al.(2017)이 제안한 Transformer는 이 두 문제를 **Self-Attention 메커니즘**으로 해결했다. 핵심 아이디어는 간단하지만 강력하다: "순차 처리를 버리고, 문장의 모든 단어 쌍 간의 관계를 한 번에 병렬로 계산하자."

**2) Self-Attention 메커니즘: Query, Key, Value**

Self-Attention의 작동 원리를 이해하기 위해, "검색(retrieval)" 비유가 유용하다. 문장 내 각 단어가 다른 단어들로부터 얼마나 정보를 가져올지(attend to)를 결정하는 과정이다.

**단계 1: Query, Key, Value 벡터 생성**

입력 문장의 각 단어 임베딩 xix_i xi​에 대해 세 가지 벡터를 생성한다:

Qi=xiWQ(Query: "내가 찾고자 하는 것")Ki=xiWK(Key: "내가 제공하는 정보")Vi=xiWV(Value: "실제 전달할 내용")\begin{align} Q_i &= x_i W^Q \quad \text{(Query: "내가 찾고자 하는 것")} \\ K_i &= x_i W^K \quad \text{(Key: "내가 제공하는 정보")} \\ V_i &= x_i W^V \quad \text{(Value: "실제 전달할 내용")} \end{align}Qi​Ki​Vi​​=xi​WQ(Query: "내가 찾고자 하는 것")=xi​WK(Key: "내가 제공하는 정보")=xi​WV(Value: "실제 전달할 내용")​​

여기서 WQ,WK,WVW^Q, W^K, W^V WQ,WK,WV는 학습 가능한 가중치 행렬이다.

**[Figure 2.1: Query, Key, Value 벡터 생성 과정 및 Self-Attention 메커니즘 도식. 입력 단어 임베딩으로부터 세 종류의 벡터가 생성되고, 이들 간의 내적과 softmax를 거쳐 최종 출력이 계산되는 전체 흐름을 시각화]**

**단계 2: Attention Score 계산**

Query와 Key의 내적(dot product)으로 단어 간 유사도를 계산한다:

score(Qi,Kj)=Qi⋅Kjdk\text{score}(Q_i, K_j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}score(Qi​,Kj​)=dk​​Qi​⋅Kj​​

여기서 dkd_k dk​는 Key 벡터의 차원이다. dk\sqrt{d_k} dk​​로 나누는 이유는 **스케일링(scaling)**이다. 내적 값이 dkd_k dk​에 비례하여 커지면 softmax의 gradient가 매우 작아져 학습이 불안정해지기 때문이다(Vaswani et al., 2017).

예를 들어, "The cat sat on the mat" 문장에서 "sat"(Query)가 각 단어(Key)와의 관련성을 계산하면 다음과 같은 점수를 얻을 수 있다:

```
sat의 Query가 보는 Key들:
The: 0.1  (관사, 낮은 관련성)
cat: 0.9  (주어, 높은 관련성)
sat: 1.0  (자기 자신)
on:  0.5  (전치사, 중간)
the: 0.1
mat: 0.7  (목적어, 높은 관련성)
```

**[Figure 2.2: "The cat sat on the mat" 문장에 대한 Attention Weight 히트맵. 각 단어(행)가 다른 단어(열)에 주목하는 정도를 색상 강도로 표현. "sat"이 "cat"과 "mat"에 높은 attention을 보이는 패턴 시각화]**

**단계 3: Softmax로 확률 분포 변환**

αij=softmax(score(Qi,Kj))=exp⁡(score(Qi,Kj))∑kexp⁡(score(Qi,Kk))\alpha_{ij} = \text{softmax}(\text{score}(Q_i, K_j)) = \frac{\exp(\text{score}(Q_i, K_j))}{\sum_{k} \exp(\text{score}(Q_i, K_k))}αij​=softmax(score(Qi​,Kj​))=∑k​exp(score(Qi​,Kk​))exp(score(Qi​,Kj​))​

이제 αij\alpha_{ij} αij​는 "단어 i가 단어 j에 얼마나 주목(attention)할지"를 나타내는 확률이다.

**단계 4: Value 가중합**

최종적으로, 각 단어는 다른 단어들의 Value를 attention 가중치로 가중 평균한다:

Attention(Qi,K,V)=∑jαijVj\text{Attention}(Q_i, K, V) = \sum_{j} \alpha_{ij} V_jAttention(Qi​,K,V)=j∑​αij​Vj​

전체 과정을 행렬 연산으로 표현하면:

Attention(Q,K,V)=softmax(QKTdk)V\boxed{\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V}Attention(Q,K,V)=softmax(dk​​QKT​)V​

이 수식이 **Scaled Dot-Product Attention**의 핵심이다.

**3) Multi-Head Attention: 다양한 관점에서 보기**

하나의 Attention만으로는 단어 간 관계의 다양한 측면을 포착하기 어렵다. 예를 들어:

- "cat sat"의 관계: 주어-동사 (syntactic)
- "cat"과 "mat"의 관계: 의미적 연관 (semantic)

Multi-Head Attention은 이를 위해 hh h개의 서로 다른 Attention을 병렬로 수행한다:

headi=Attention(QWiQ,KWiK,VWiV)MultiHead(Q,K,V)=Concat(head1,…,headh)WO\begin{align} \text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\ \text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \end{align}headi​MultiHead(Q,K,V)​=Attention(QWiQ​,KWiK​,VWiV​)=Concat(head1​,…,headh​)WO​​

각 head는 서로 다른 WiQ,WiK,WiVW_i^Q, W_i^K, W_i^V WiQ​,WiK​,WiV​ 행렬을 사용하므로, 다른 "관점"으로 문장을 본다. GPT-3는 96개의 head를 사용하여 문법, 의미, 담화 구조 등 다양한 언어적 측면을 동시에 학습한다.

**[Figure 2.3: Multi-Head Attention 구조도. 입력이 h개의 병렬 attention head로 분기되고, 각 head가 독립적으로 QKV 변환 및 attention을 수행한 후 concatenation되어 최종 출력을 생성하는 과정 시각화. 예시로 Head 1은 문법 관계(주어-동사), Head 2는 의미 관계(장소-주체), Head 3은 수식 관계(한정사-명사)에 주목하는 패턴 표시]**

**시각화 예시:**

```
Head 1: 문법 관계에 주목
  "cat" → "sat" (0.9)  주어-동사

Head 2: 의미 관계에 주목
  "cat" → "mat" (0.8)  장소-주체

Head 3: 수식 관계에 주목
  "the" → "mat" (0.9)  한정사-명사
```

---

**4) Positional Encoding: 순서 정보의 주입**

Self-Attention은 모든 단어 쌍을 동시에 보기 때문에, 단어의 **순서 정보가 사라진다**는 문제가 있다. "cat sat on mat"과 "mat on sat cat"을 구분할 수 없다.

Transformer는 이를 해결하기 위해 각 위치에 고유한 **Positional Encoding**을 더한다:

PE(pos,2i)=sin⁡(pos100002i/dmodel)PE(pos,2i+1)=cos⁡(pos100002i/dmodel)\begin{align} PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \\ PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) \end{align}PE(pos,2i)​PE(pos,2i+1)​​=sin(100002i/dmodel​pos​)=cos(100002i/dmodel​pos​)​​

여기서 pospos pos는 단어의 위치(0, 1, 2, ...), ii i는 차원 인덱스이다. Sin과 Cos 함수를 사용하는 이유는:

1. **상대적 위치 학습 가능**: PEpos+kPE_{pos+k} PEpos+k​는 PEposPE_{pos} PEpos​의 선형 함수로 표현 가능
2. **긴 문장 일반화**: 학습 시보다 긴 문장에도 적용 가능

**[Figure 2.4: Positional Encoding 시각화. 위치(pos)에 따른 sin/cos 함수 값의 변화를 히트맵으로 표현. 서로 다른 차원(i)에서 다른 주파수의 파형이 생성되어 각 위치에 고유한 인코딩 패턴이 만들어지는 모습 시각화]**

---

**5) Decoder-only 아키텍처: GPT의 선택**

원래 Transformer는 Encoder-Decoder 구조(기계번역용)였으나, GPT 시리즈는 **Decoder-only** 구조를 채택했다. 이는 자기회귀적(auto-regressive) 언어 생성에 최적화된 설계로, 이전 단어들만 보고 다음 단어를 예측하는 방식이다. Masked Self-Attention을 통해 미래 토큰을 보지 못하도록 제한함으로써, 대규모 텍스트 데이터로 비지도 학습이 가능하다는 장점이 있다.

이러한 Transformer 아키텍처의 혁신은 2020년대 대규모 언어모델의 폭발적 성장을 가능하게 했으며, 본 연구에서 다루는 모든 오픈소스 LLM(Gemma, Qwen, EXAONE, gpt-oss) 역시 Transformer 기반이다.
### **2.1.3 대규모 언어모델(LLM)의 능력과 한계**

**1) LLM의 주요 능력**

Transformer 아키텍처를 기반으로 수십억~수천억 개의 파라미터로 확장된 대규모 언어모델은 자연어 이해와 생성에서 인간 수준에 근접하거나 일부 작업에서 이를 능가하는 성능을 보인다. 특히, 사전학습(pre-training) 과정에서 방대한 텍스트 데이터로부터 학습한 지식은 다양한 다운스트림 작업(downstream tasks)에 전이(transfer)된다.

**컨텍스트 이해(In-Context Learning)**

LLM의 가장 획기적인 능력은 명시적인 파라미터 업데이트 없이도 프롬프트에 제공된 맥락만으로 새로운 작업을 수행할 수 있다는 점이다. Brown et al.(2020)은 GPT-3가 몇 가지 예시(demonstrations)만으로 번역, 질의응답, 산술 연산 등 다양한 작업을 수행함을 보였다. 이는 모델이 학습 데이터로부터 메타 학습(meta-learning) 능력을 습득했음을 시사한다.

**Few-shot Learning**

전통적인 머신러닝은 특정 작업을 위해 수천~수만 개의 레이블된 데이터가 필요했다. 반면 LLM은 단 몇 개(few-shot)의 예시만으로도 새로운 작업의 패턴을 파악한다. 예를 들어:

```
[프롬프트]
한국어를 영어로 번역하세요.

입력: 안녕하세요
출력: Hello

입력: 감사합니다
출력: Thank you

입력: 좋은 하루 되세요
출력: [모델이 생성]
```

이러한 능력은 도메인 특화 작업에서도 유용하다. 공공 민원 상담의 경우, 소수의 상담 예시만 제공해도 유사한 응답 패턴을 생성할 수 있다.

**창의적 텍스트 생성**

LLM은 단순한 정보 검색을 넘어 창의적인 텍스트를 생성한다. 시, 소설, 코드, 이메일 초안 등 다양한 형태의 콘텐츠를 인간이 제시한 제약 조건과 스타일에 맞춰 작성할 수 있다. 이는 모델이 언어의 통계적 패턴뿐만 아니라 담화 구조(discourse structure)와 화용론적 측면(pragmatics)까지 학습했음을 의미한다.

**[Figure 2.5: LLM의 In-Context Learning 메커니즘. 프롬프트에 제공된 few-shot 예시들이 어떻게 모델의 attention 패턴에 영향을 미치는지 시각화. 예시 입력-출력 쌍과 테스트 입력 간의 attention weight 분포 표시]**

**2) LLM의 근본적 한계**

그러나 LLM의 인상적인 성능에도 불구하고, 실용적 활용을 제한하는 구조적 한계들이 존재한다.

**환각(Hallucination): 사실이 아닌 정보의 생성**

LLM의 가장 심각한 문제는 그럴듯하지만 사실이 아닌 정보를 자신있게 생성하는 경향이다(Ji et al., 2023). 이는 모델이 다음 토큰의 확률 분포를 학습했을 뿐, "진실"에 대한 명시적 표현을 가지지 않기 때문이다.

예를 들어, "서울시 다산콜센터의 설립 연도는?"이라는 질문에 모델이 학습 데이터에 해당 정보가 없으면, 유사한 기관의 설립 연도 패턴을 기반으로 그럴듯한 연도를 지어낼 수 있다. 공공 서비스나 의료, 법률 등 사실 정확성이 중요한 도메인에서는 치명적이다.

**[Figure 2.6: LLM 환각의 메커니즘. 학습 데이터에 존재하지 않는 질문에 대해 모델이 유사 패턴을 기반으로 그럴듯한 답변을 생성하는 과정 도식화. 좌측은 학습 데이터의 패턴, 우측은 생성된 환각 응답]**

**지식 시점 고정(Knowledge Cutoff)**

LLM의 지식은 사전학습 데이터의 시점에 고정되어 있다. 예를 들어 2023년 9월까지의 데이터로 학습된 모델은 그 이후의 정책 변경, 새로운 법규, 최신 연구 결과 등을 알 수 없다. 공공부문의 경우 정책과 제도가 빈번히 변경되므로, 모델을 재학습하지 않고는 최신 정보를 제공할 수 없다는 근본적 한계가 있다.

또한, 수백억~수천억 개의 파라미터를 가진 LLM의 재학습은 막대한 컴퓨팅 자원과 비용을 요구한다. GPT-3의 학습 비용은 약 460만 달러로 추정되며(Li et al., 2023), 이는 지속적인 지식 업데이트를 현실적으로 불가능하게 만든다.

**도메인 특화 지식의 부족**

LLM은 웹 크롤링 데이터와 같은 일반적인 텍스트로 학습되므로, 특정 도메인의 전문 지식은 상대적으로 부족하다. 의료 진단, 법률 해석, 공공 행정의 세부 절차 등은 공개 웹 데이터에 충분히 존재하지 않거나, 존재하더라도 일반 텍스트에 비해 비중이 낮아 제대로 학습되지 않는다.

다산콜센터의 경우, 서울시 자치법규, 내부 업무 매뉴얼, 민원 처리 절차 등 비공개 또는 제한적으로 공개된 정보가 많다. 이러한 도메인 특화 지식 없이는 정확한 민원 상담이 불가능하다.

**[Figure 2.7: LLM의 한계를 보완하는 외부 지식 통합의 필요성. 중앙의 LLM(Parametric Memory)과 주변의 외부 지식 소스들(Non-parametric Memory)을 연결하는 화살표로 RAG 개념 시각화. 외부 소스: 최신 뉴스/정책, 도메인 특화 문서, 내부 데이터베이스 등]**

**3) 외부 지식 통합의 필요성**

이러한 한계들은 LLM의 **Parametric Memory**(파라미터에 내장된 지식)만으로는 실용적 시스템을 구축할 수 없음을 보여준다. 해결책은 **Non-parametric Memory**, 즉 외부 지식 베이스를 동적으로 참조하는 하이브리드 접근이다.

Lewis et al.(2020)이 제안한 Retrieval-Augmented Generation(RAG)은 이러한 아이디어의 선구적 연구로, 질의에 관련된 문서를 먼저 검색한 후 이를 컨텍스트로 제공하여 LLM의 생성 품질을 향상시킨다. 이 접근법은 다음과 같은 장점을 가진다:

1. **지식 업데이트 용이**: 외부 데이터베이스만 갱신하면 되므로 모델 재학습 불필요
2. **환각 감소**: 실제 문서를 참조하므로 사실 기반 응답 가능
3. **도메인 특화**: 도메인 특화 문서를 검색 대상으로 추가 가능
4. **출처 제공**: 응답의 근거가 되는 문서를 명시하여 신뢰성 향상

그러나 초기 RAG 시스템은 단순한 벡터 유사도 검색에 의존하여, 복잡한 추론이나 다중 정보 간의 관계 파악에는 한계가 있었다. 이는 다음 절에서 다룰 Graph RAG의 등장 배경이 된다.

### **2.1.4 Retrieval-Augmented Generation (RAG)의 등장**

**1) RAG의 핵심 개념: Parametric vs Non-parametric Memory**

LLM의 한계를 극복하기 위한 근본적 아이디어는 모델의 **Parametric Memory**(파라미터에 저장된 지식)와 **Non-parametric Memory**(외부 데이터베이스의 지식)를 결합하는 것이다.

Parametric Memory는 사전학습 과정에서 수백억 개의 가중치에 압축되어 저장된 지식으로, 빠른 추론이 가능하지만 업데이트가 어렵고 출처를 명시할 수 없다. 반면 Non-parametric Memory는 외부 문서 저장소로, 언제든지 추가/수정이 가능하며 검색된 문서를 통해 근거를 제시할 수 있다.

RAG(Retrieval-Augmented Generation)는 이 두 메모리 시스템을 통합한다(Lewis et al., 2020). 핵심 아이디어는 간단하다: "질문에 답하기 전에 먼저 관련 문서를 찾아보고(Retrieval), 그 정보를 바탕으로 답변을 생성하자(Generation)."

**[Figure 2.8: Parametric vs Non-parametric Memory 비교 다이어그램. 좌측에 LLM(뇌 모양 아이콘)과 고정된 파라미터, 우측에 데이터베이스(문서 아이콘)와 동적 업데이트 화살표. 중앙에 RAG가 두 메모리를 연결하는 구조 시각화]**

**2) Naive RAG 파이프라인**

초기 RAG 시스템, 즉 "Naive RAG"는 다음 네 단계로 구성된다:

**Step 1: Chunking (문서 분할)**

대용량 문서를 모델의 컨텍스트 윈도우에 맞게 작은 단위로 분할한다. 일반적으로 문장 단위 또는 고정 토큰 길이(예: 512 tokens)로 나눈다. 예를 들어:

```
원본 문서: "서울시 다산콜센터(120)는 2007년 설립되었으며, 
시민의 민원을 365일 24시간 처리합니다. 주요 업무는..."

↓ Chunking

Chunk 1: "서울시 다산콜센터(120)는 2007년 설립되었으며, 
         시민의 민원을 365일 24시간 처리합니다."
Chunk 2: "주요 업무는 시정 문의, 불편 신고, 생활 정보 제공..."
```

**Step 2: Embedding (벡터 변환)**

각 청크를 고차원 벡터 공간으로 임베딩한다. 일반적으로 BERT, Sentence-BERT, OpenAI의 text-embedding-ada-002 등의 사전학습된 임베딩 모델을 사용한다. 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치된다.

Chunki→Embedding Modelvi∈Rd\text{Chunk}_i \xrightarrow{\text{Embedding Model}} \mathbf{v}_i \in \mathbb{R}^dChunki​Embedding Model​vi​∈Rd

여기서 dd d는 일반적으로 768(BERT) 또는 1536(OpenAI) 차원이다.

**Step 3: Vector Search (유사도 기반 검색)**

사용자 질의(query)를 동일한 임베딩 모델로 벡터화한 후, 코사인 유사도(cosine similarity)를 통해 가장 관련성 높은 top-k 청크를 검색한다:

similarity(q,vi)=q⋅vi∣∣q∣∣⋅∣∣vi∣∣\text{similarity}(\mathbf{q}, \mathbf{v}_i) = \frac{\mathbf{q} \cdot \mathbf{v}_i}{||\mathbf{q}|| \cdot ||\mathbf{v}_i||}similarity(q,vi​)=∣∣q∣∣⋅∣∣vi​∣∣q⋅vi​​

예를 들어, "다산콜센터 운영 시간은?"이라는 질의는 "365일 24시간"을 포함한 청크와 높은 유사도를 가진다.

**Step 4: Generation (컨텍스트 기반 생성)**

검색된 청크들을 LLM의 프롬프트에 컨텍스트로 삽입하여 답변을 생성한다:

```
[시스템 프롬프트]
다음 정보를 바탕으로 질문에 답하세요.

[검색된 컨텍스트]
서울시 다산콜센터(120)는 2007년 설립되었으며, 
시민의 민원을 365일 24시간 처리합니다.

[사용자 질의]
다산콜센터 운영 시간은?

[모델 생성]
다산콜센터는 365일 24시간 운영됩니다.
```

**[Figure 2.9: Naive RAG 파이프라인 플로우차트. 좌측부터 순서대로: 문서 → Chunking → Embedding → Vector DB 저장 → 사용자 질의 입력 → 질의 임베딩 → Vector Search (top-k) → 검색된 청크들 → LLM 프롬프트 구성 → 최종 답변 생성. 각 단계 사이에 화살표와 간단한 아이콘 표시]**

**3) RAG의 실무 활용 사례**

RAG는 2023년 이후 엔터프라이즈 AI의 사실상 표준(de facto standard)으로 자리잡았다. 주요 활용 사례는 다음과 같다:

- **GitHub Copilot**: 개발자의 코드베이스를 검색하여 프로젝트 맥락에 맞는 코드 제안
- **Notion AI**: 사용자의 워크스페이스 내 문서를 검색하여 개인화된 답변 제공
- **Customer Support Chatbots**: 제품 매뉴얼, FAQ, 과거 상담 이력을 검색하여 일관된 응답 생성

2024년 Gartner 조사에 따르면, 엔터프라이즈 AI 도입 조직의 65% 이상이 RAG 기반 시스템을 운영 중이거나 도입을 계획하고 있다. 공공부문에서도 민원 상담, 정책 질의응답, 내부 문서 검색 등에 RAG 활용이 증가하는 추세이다.

**4) RAG의 장점과 한계**

**장점:**

1. **지식 업데이트 용이**: 외부 문서만 추가/수정하면 되므로 모델 재학습 불필요. 다산콜센터의 경우, 새로운 정책이 발표되면 해당 문서만 데이터베이스에 추가하면 즉시 반영된다.
2. **환각 감소**: 실제 문서를 참조하므로 근거 없는 정보 생성이 크게 줄어든다. 검색된 컨텍스트에 없는 내용은 "정보가 부족합니다"라고 답할 수 있다.
3. **출처 제공**: 응답의 근거가 된 문서를 사용자에게 제시하여 신뢰성을 높인다. 공공 서비스에서 특히 중요한 요소이다.
4. **도메인 특화 가능**: 범용 LLM에 도메인 특화 문서를 외부 지식으로 제공하여 전문성을 확보한다.

**한계:**

그러나 Naive RAG는 **평면적(flat) 검색**에 의존한다는 근본적 한계가 있다. 각 청크는 독립적으로 임베딩되고 검색되므로, 청크 간의 **관계(relation)**나 **구조(structure)**를 포착하지 못한다.

예를 들어, "다산콜센터와 자치구청의 민원 처리 역할 분담은?"이라는 질문은 두 기관의 **관계**를 묻는 것이지만, 벡터 검색은 각 기관을 개별적으로 언급한 청크들을 찾을 뿐이다. "A는 B를 관리한다", "B는 C에 속한다"와 같은 다중 홉(multi-hop) 추론이 필요한 질의에서는 성능이 저하된다.

또한, 전체 코퍼스에 대한 요약이나 통합적 분석("서울시 민원 처리 체계의 전반적 특징은?")과 같은 **글로벌 질의**에는 적합하지 않다. 상위 k개 청크만 검색하므로 전체적인 그림을 파악하기 어렵다.

**[Figure 2.10: Naive RAG의 한계 시각화. 좌측에 독립적인 청크들(평면적 배치), 우측에 "다산콜센터 ← 관리 ← 서울시 → 운영 → 자치구청" 같은 관계 구조가 필요한 질의 예시. 중앙에 "관계 정보 손실" 표시로 한계 강조]**

**5) Graph RAG의 동기: 구조화된 지식의 필요성**

이러한 한계는 지식을 단순한 텍스트 청크의 집합이 아닌, **엔티티(entity)와 관계(relation)로 구성된 그래프**로 표현해야 한다는 인사이트로 이어진다.

Microsoft Research는 2024년 "From Local to Global: A Graph RAG Approach"에서 이 아이디어를 구체화했다. 텍스트로부터 엔티티(예: "다산콜센터", "서울시", "민원")와 그들 간의 관계(예: "운영", "처리")를 추출하여 지식 그래프(Knowledge Graph)를 구축하고, 이를 RAG의 검색 대상으로 활용하는 것이다.

Graph RAG는 벡터 검색의 의미적 유연성과 그래프 순회의 구조적 추론 능력을 결합하여, Naive RAG가 어려워하는 복잡한 질의에서도 우수한 성능을 보인다. 이는 다음 절에서 상세히 다룰 지식 그래프의 이론적 기초와 연결된다.


## 제1장 서론 (7-8페이지)

### 1.1 연구 배경 및 필요성 (2.5p)

- 공공부문 AI 도입 증가와 비용 문제
    
- 다산콜센터의 AI 혁신 필요성
    
- 온프레미스 환경의 중요성 (데이터 주권, 비용 절감)

### 1.2 연구의 목적 (1.5p)

- **주 목적**: 상용 GPU 환경에서 오픈소스 LLM + GraphRAG의 실무 적용 가능성 검증
    
- **세부 목표**:
    
    - 계층적 RAG 모델의 성능 비교
        
    - 한국어 민원 데이터 최적화 전략 도출
        
    - 비용 효율성 및 확산 가능성 분석
        

### 1.3 연구의 내용 및 범위 (1.5p)

- **비교 대상**: 4개 오픈소스 모델 × 4단계 RAG 방식 = 16개 조합
    
- **평가**: 정량(자동+RAG지표), 정성(사례분석), 비용 분석
    
- **환경**: RTX 3090Ti 기반 온프레미스
    

### 1.4 연구의 차별점과 기여도 ⭐ (1.5p)

#### 1.4.1 학술적 기여

- 계층적 RAG 설계를 통한 구성요소별 기여도 정량화
    
- 한국어 민원 도메인에 특화된 평가 체계 제시
    

#### 1.4.2 실무적 기여

- 공공기관 도입 가능한 비용 효율적 솔루션 제시
    
- 오픈소스 기반 AI 기술 주권 확보 방안
    

### 1.5 논문 구성 (0.5p)

---

## 제2장 이론적 배경 및 관련 연구 (15-17페이지)

### 2.1 기술적 배경 (7-8p)

#### 2.1.1 인공지능과 자연어처리의 발전 🆕 (1.5p)

- **AI 발전 단계**: 규칙 기반 → 통계 기반 → 딥러닝 → LLM
    
- Transformer 아키텍처의 혁신
    
- Self-Attention 메커니즘 (수식 + 그림)
    

#### 2.1.2 대규모 언어모델(LLM)의 진화와 한계 (1.5p)

- 사전학습-파인튜닝 패러다임
    
- **LLM의 능력**: 문맥 이해, 추론, 생성
    
- **한계점**: 환각(Hallucination), 지식 업데이트 어려움, 도메인 특화 부족
    

#### 2.1.3 Retrieval-Augmented Generation (RAG) 기술 (2p)

- **Basic RAG**: Vector Search 기반 검색 증강
    
    - **아키텍처**: Indexing → Retrieval → Generation
        
    - 장점과 한계
        
- **Advanced RAG**: Hybrid Search, Re-ranking, Query Rewriting
    
- **평가**: RAG는 LLM의 한계를 극복하는 핵심 솔루션
    

#### 2.1.4 Graph RAG 및 최신 변형 기술 (2p)

- **Graph RAG (Microsoft, 2024)**
    
    - Knowledge Graph 기반 정보 구조화
        
    - Community Detection을 통한 계층적 요약
        
    - Local/Global Search 전략
        
- PathRAG, LightRAG 등 최신 변형
    
- **기술적 우위성**: 다중 홉 추론, 맥락 이해 향상 (15-30% 성능 개선)
    

### 2.2 관련 연구 (3-4p)

#### 2.2.1 RAG 시스템 성능 비교 연구 (1.5p)

- **국외**: GraphRAG vs RAG 비교 연구 (Edge et al., 2024)
    
- **국내**: 한국어 RAG 시스템 연구 동향
    
- **연구 공백**: 공공 민원 도메인 RAG 연구 부재
    

#### 2.2.2 공공부문 AI 챗봇 연구 동향 (1p)

- 국내 공공 챗봇 현황 (행정안전부, 2024)
    
- **문제점**: 상용 API 의존, 고비용 구조, 보안 이슈
    

#### 2.2.3 한국어 특화 LLM 및 RAG 연구 (0.5p)

- EXAONE, Polyglot-Ko 등 국산 LLM 연구
    
- 한국어 임베딩 모델 연구
    

### 2.3 연구 대상 도메인 분석 (2p)

#### 2.3.1 다산콜센터 운영 현황 (1p)

- 일 평균 4,000건 민원 처리 (2024년 기준)
    
- 상담원 65명 운영
    
- **AI 도입 필요성**: 반복 질의 자동화, 상담 품질 향상
    

#### 2.3.2 민원 데이터의 특수성 및 도전과제 (1p)

- **특수성**: 구어체, 감정 표현, 행정 전문 용어, 정책 간 연관성
    
- **도전과제**: 데이터 비정형성, 빈번한 정책 변경, 개인정보 처리
    
- **본 연구의 접근**: 이러한 특수성을 해결하기 위한 Domain-Adaptive RAG 설계
    

### 2.4 오픈소스 LLM 및 프레임워크 동향 (2-3p)

#### 2.4.1 연구 대상 모델 특성 분석 (1.5p)

- **gpt-oss-20b (OpenAI, 2025)**: MoE 아키텍처, CoT 추론
    
- **Gemma 3 12B (Google, 2025)**: 다국어 성능 강화
    
- **Qwen 3 8B (Alibaba, 2025)**: 컨텍스트 이해 우수
    
- **EXAONE 7.8B (LG, 2024)**: 한국어 특화
    

#### 2.4.2 온프레미스 구축 환경 분석 (1p)

- RTX 3090Ti (24GB VRAM) 기반 운영 가능성
    
- 양자화 기술 (4-bit/8-bit) 적용
    
- **추론 속도**: 초당 20-50 토큰 생성
    

---

## 제3장 연구 설계 및 방법론 (14-16페이지)

### 3.1 계층적 실험 설계 ⭐ (3-4p)

#### 3.1.1 실험 설계 철학 (0.5p)

- **계층적 접근**: 각 단계에서 추가 요소의 기여도 측정
    
- **목표**: "어떤 구성요소가 얼마나 성능에 기여하는가" 정량화
    

#### 3.1.2 4단계 실험 그룹 설계 (2.5p)

|단계|모델 구성|비교 목적|기대 성능|
|---|---|---|---|
|**Baseline**|Pure LLM (RAG 없음)|기본 성능 측정|낮음|
|**Stage 1**|LLM + Basic RAG (Vector Search)|RAG의 기본 효과|중간|
|**Stage 2**|LLM + Advanced Retriever (Hybrid Search + Re-rank)|검색 고도화 효과|높음|
|**Stage 3**|LLM + Graph RAG (Knowledge Graph)|구조화된 지식 효과|최고|

Sheets로 내보내기

- **실험 조합**: 4개 모델 × 4단계 = 16개 실험 설정
    

#### 3.1.3 통제 변수 및 독립 변수 (1p)

- **통제**: Temperature=0.3, Max Tokens=512, Context Window=8K
    
- **독립 변수**: 모델 종류, RAG 방식
    
- **종속 변수**: 정확도, 충실성, 관련성, 추론 시간, 비용
    

### 3.2 데이터셋 구축 및 전처리 (3-4p)

#### 3.2.1 지식 베이스 구축 (1.5p)

- **데이터 소스**: 서울시 다산콜센터 FAQ API (3,000+ 건)
    
- **수집 기간**: 2024년 1-12월
    
- **데이터 구조**: 질문-답변-카테고리-정책명-관련법령
    

#### 3.2.2 평가 데이터셋 설계 (1p)

- **소스**: AI Hub 다산콜센터 Q&A 데이터셋
    
- **규모**: 10,000개 질의-응답 쌍
    
- **분할**: Train(70%) / Validation(15%) / Test(15%)
    
- **난이도 분류**: 단순(50%), 중간(30%), 복잡(20%)
    

#### 3.2.3 민원 데이터 특화 전처리 전략 🆕 (1.5p)

```
# 전처리 파이프라인:
1. 개인정보 비식별화 (이름, 주민번호 → <PERSON>, <RRN>)
2. 구어체 정제 ("그니까" → "그러니까")
3. 행정 용어 표준화 ("120다산콜" → "다산콜센터")
4. Chunking: Semantic Chunking (512 토큰 단위)
5. Embedding: multilingual-e5-large-instruct
```

### 3.3 RAG 시스템 아키텍처 (3-4p)

#### 3.3.1 Retriever 구성 (1.5p)

- **Basic RAG**: FAISS Vector DB + Top-5 검색
    
- **Advanced RAG**: Hybrid Search (BM25 + Vector) + Re-ranking
    
- **Graph RAG**: Neo4j Knowledge Graph + Community Detection
    

#### 3.3.2 Generator 구성 (1p)

- 4개 오픈소스 LLM (3.1.2절 참조)
    
- 프롬프트 템플릿 표준화
    
- Response Generation 설정
    

#### 3.3.3 시스템 통합 및 최적화 (0.5p)

- LangChain/LlamaIndex 프레임워크 활용
    
- End-to-End 파이프라인 구축
    

### 3.4 평가 지표 및 방법 ⭐ (3-4p)

#### 3.4.1 자동 평가 지표 (1p)

- **ROUGE-L**: 답변 중첩도
    
- **BERTScore**: 의미적 유사도
    
- **Exact Match (EM)**: 정확한 일치율
    

#### 3.4.2 RAG 특화 평가 지표 (1.5p)

- **Faithfulness**: 검색 문서와 답변의 일치도
    
- **Context Precision**: 검색된 문서의 정확성
    
- **Context Recall**: 필요한 문서 검색률
    
- **Answer Relevancy**: 답변의 질의 적합성
    
- **측정 도구**: RAGAS Framework
    

#### 3.4.3 인간 평가 기준 및 방법 (1p)

- **평가자**: 다산콜센터 상담원 5명
    
- **평가 항목**: 정확성(5점), 유용성(5점), 자연스러움(5점)
    
- **샘플 크기**: 각 모델당 100개 무작위 샘플
    
- **Inter-rater Reliability**: Cohen's Kappa 계산
    

### 3.5 실험 환경 및 하이퍼파라미터 설정 (1.5p)

#### 3.5.1 하드웨어 및 소프트웨어 환경 (0.7p)

- **GPU**: NVIDIA RTX 3090Ti (24GB VRAM)
    
- **CPU**: AMD Ryzen 9 7950X
    
- **RAM**: 64GB DDR5
    
- **OS**: Ubuntu 24.04 LTS
    
- **프레임워크**: LangChain 0.2.x, LlamaIndex 0.11.x
    

#### 3.5.2 모델별 하이퍼파라미터 설정 (0.5p)

- [표: 4개 모델의 상세 설정]
    

#### 3.5.3 재현가능성을 위한 실험 설정 (0.3p)

- **Random Seed 고정**: 42
    
- **코드 공개**: GitHub Repository
    
- **데이터셋 공개 계획**
    

### 3.6 윤리적 고려사항 🆕 (1.5p)

#### 3.6.1 개인정보 비식별화 처리 (0.5p)

- 정보통신망법 준수
    
- 이름, 주민번호, 주소 등 마스킹 처리
    

#### 3.6.2 데이터 사용 승인 및 IRB (0.5p)

- 서울시 데이터 사용 승인 획득
    
- 기관 IRB 심의 통과 (승인번호: XXX)
    

#### 3.6.3 AI 편향성 완화 전략 (0.5p)

- 다양한 민원 유형 균등 포함
    
- 성별, 연령, 지역 편향 검증
    

---

## 제4장 실험 결과 및 분석 (20-24페이지) 🌟

### 4.1 계층적 모델 성능 비교 분석 (6-7p)

#### 4.1.1 정량적 평가 결과 (3p)

- **전체 성능 비교 (예상)** | 모델 | Baseline | Basic RAG | Advanced RAG | Graph RAG | | :--- | :--- | :--- | :--- | :--- | | **gpt-oss-20b** | 78% | 88% (+10%p) | 93% (+5%p) | 96% (+3%p) | | **EXAONE 7.8B** | 75% | 86% (+11%p) | 91% (+5%p) | 95% (+4%p) | | **Gemma 3 12B** | 76% | 85% (+9%p) | 89% (+4%p) | 92% (+3%p) | | **Qwen 3 8B** | 77% | 87% (+10%p) | 90% (+3%p) | 93% (+3%p) |
    
- **주요 발견**:
    
    - Basic RAG 도입으로 평균 10%p 성능 향상
        
    - Graph RAG에서 EXAONE이 최고 성능 (한국어 특화)
        
    - 복잡한 민원일수록 Graph RAG 효과 ↑ (30%p 향상)
        
- [그래프: Radar Chart, Bar Chart, Heatmap]
    

#### 4.1.2 통계적 유의성 검증 (1.5p)

- **검정 방법**: Paired t-test, Wilcoxon signed-rank test
    
- **유의수준**: p < 0.05
    
- **결과**: 모든 RAG 단계 향상이 통계적으로 유의미
    

#### 4.1.3 모델별 성능 특성 분석 (1.5p)

- **gpt-oss-20b**: 복잡한 추론에 강점
    
- **EXAONE**: 한국어 행정 용어 이해 우수
    
- **Gemma/Qwen**: 비용 효율적 균형
    

### 4.2 Ablation Study: 구성 요소별 기여도 분석 ⭐ (5-6p)

#### 4.2.1 Retriever 구성 요소 영향도 (2p)

- **실험 설정**:
    
    - 실험 1: Vector Search only
        
    - 실험 2: BM25 only
        
    - 실험 3: Hybrid (Vector + BM25)
        
    - 실험 4: Hybrid + Re-ranking
        
    - 실험 5: Graph RAG
        
- [표: 각 구성요소의 성능 기여도]
    
- **주요 발견**:
    
    - Re-ranking이 5%p 추가 향상
        
    - Graph 구조가 다중 홉 질의에서 15%p 향상
        

#### 4.2.2 Chunking 전략 영향도 (1.5p)

- 고정 크기 (512 토큰) vs Semantic Chunking
    
- **결과**: Semantic Chunking이 3%p 우수
    

#### 4.2.3 Embedding 모델 선택 영향도 (1.5p)

- OpenAI text-embedding-3 vs e5-large vs bge-m3
    
- **결과**: multilingual-e5가 한국어 민원에 최적
    

### 4.3 정성적 분석: 사례 기반 심층 분석 🆕 (4-5p)

#### 4.3.1 오류 유형 분류 및 패턴 분석 (2p)

- **오류 분류 체계 (100개 오답 사례 분석)**: | 오류 유형 | 발생 빈도 | 주요 원인 | 예시 | | :--- | :--- | :--- | :--- | | **검색 실패** | 35% | 관련 문서 없음 | "신설 정책 문의" | | **환각** | 25% | LLM 지식 의존 | "2025년 법 개정사항" | | **맥락 오해** | 20% | 복잡한 다중 의도 | "A도 되고 B도 되나요?" | | **형식 오류** | 15% | 프롬프트 이탈 | 답변 형식 불일치 | | **기타** | 5% | - | - |
    
- **개선 방안**:
    
    - **검색 실패** → 지식 베이스 확장
        
    - **환각** → Graph RAG 강제 참조
        
    - **맥락 오해** → Query Decomposition
        

#### 4.3.2 성공 사례 분석 (1.5p)

- **사례 1: 복잡한 다중 정책 질의**
    
    - **질의**: "1인 가구가 청년 전월세 보증금 대출 받으려면 소득 요건과 나이 제한이 어떻게 되나요?"
        
    - **Graph RAG 답변**:
        
        1. [정책A: 청년 전월세 대출] 노드 검색
            
        2. 관계 추적: 연결된 [소득 요건], [나이 제한] 노드
            
        3. [1인 가구 정책]과의 교차점 확인
            
        4. 종합 답변 생성: "만 19-34세, 연소득 5천만원 이하..."
            
    - ✅ **정확도**: 95% (인간 평가)
        

#### 4.3.3 도메인 특화 이슈 분석 (0.5p)

- 민원 데이터 특유의 구어체 처리
    
- 행정 전문 용어 이해도
    

### 4.4 비용 및 효율성 분석 (3-4p)

#### 4.4.1 추론 시간 및 처리량 비교 (1.5p)

- [표: 모델별 추론 속도]
    
- **Baseline**: 평균 0.8초/질의
    
- **Graph RAG**: 평균 2.3초/질의
    
- **처리량**: 일 4,000건 처리 가능 (병렬 처리 시)
    

#### 4.4.2 컴퓨팅 자원 사용량 분석 (1p)

- **GPU 메모리**: 평균 18GB 사용 (RTX 3090Ti 내 운영 가능)
    
- **전력 소비**: 평균 320W
    

#### 4.4.3 비용-효과 분석 (TCO/ROI) (1.5p)

- **TCO 비교 (월 1,000만 요청 기준)**: | 항목 | 상용 API | 온프레미스 (본 연구) | | :--- | :--- | :--- | | **초기 투자** | 0원 | 700만원 (HW) | | **월 운영비** | 500만원 | 40만원 (전기+유지보수) | | **연간 비용** | 6,000만원 | 1,180만원 | | **절감률** | - | **80.3%** |
    
- **ROI**: 1.5개월 내 손익분기점 도달
    

### 4.5 종합 분석 및 고찰 (2-3p)

#### 4.5.1 주요 발견사항 종합 (1p)

- Graph RAG는 복잡한 민원에서 15-30% 성능 향상
    
- EXAONE + Graph RAG 조합이 한국어 민원 최적
    
- 온프레미스 환경으로 연간 5천만원 절감 가능
    

#### 4.5.2 실무 적용 가능성 평가 (1p)

- **즉시 적용 가능**: Basic/Advanced RAG
    
- **단계적 도입 권장**: Graph RAG (지식 그래프 구축 비용)
    
- **예상 효과**: 상담원 업무 30% 경감, 응답 시간 50% 단축
    

#### 4.5.3 연구 가설 검증 (0.5p)

- ✅ **가설 1**: "Graph RAG가 Naive RAG보다 우수" → 입증
    
- ✅ **가설 2**: "온프레미스 환경 실현 가능" → 입증
    
- ✅ **가설 3**: "비용 절감 효과" → 입증 (80% 절감)
    

---

## 제5장 결론 및 제언 (6-7페이지)

### 5.1 연구 결과 요약 (1.5p)

- 계층적 RAG 설계를 통한 16개 모델 비교
    
- Graph RAG의 우수성 실증적 입증
    
- 온프레미스 환경 실현 가능성 확인
    

### 5.2 학술적 기여도 및 실무적 시사점 (2.5p)

#### 5.2.1 이론적 기여 (1p)

- 계층적 RAG 평가 체계 제시
    
- 한국어 민원 도메인 RAG 벤치마크 구축
    
- Ablation Study를 통한 구성요소 기여도 정량화
    

#### 5.2.2 실무적 시사점 (1p)

- 공공기관 AI 도입 실행 가능 모델 제시
    
- 비용 절감 및 데이터 주권 확보 방안
    
- 단계적 도입 로드맵 제공
    

#### 5.2.3 공공부문 확산 전략 (0.5p)

- **1단계 (즉시)**: Basic RAG 도입
    
- **2단계 (6개월)**: Advanced RAG 고도화
    
- **3단계 (1년)**: Graph RAG 확산
    

### 5.3 연구의 한계 및 향후 연구 방향 (2p)

#### 5.3.1 연구의 한계점 (0.7p)

- 단일 도메인(다산콜센터) 평가
    
- 평가 데이터 규모 제한 (10,000건)
    
- 6개월 실험 기간의 제약
    

#### 5.3.2 향후 연구 방향 (1p)

- **멀티모달 확장**: 음성, 이미지 민원 처리
    
- **실시간 학습**: Incremental Learning 적용
    
- **타 공공기관 확장**: 국민연금, 건강보험 등
    
- **다국어 지원**: 외국인 민원 대응
    

#### 5.3.3 장기 연구 로드맵 (0.3p)

- **2025년**: 다산콜센터 실증 배포
    
- **2026년**: 서울시 전역 확산
    
- **2027년**: 전국 공공기관 표준 모델화
    

---

## 참고문헌 (3-4페이지)

- 50-80개 참고문헌 (최신 논문 위주)
    
- APA 7th Edition 형식
    

---

## 부록 🆕 (5-7페이지)

### A. 주요 기술 용어 정의 (1p)

- RAG, Graph RAG, Embedding, Chunking 등
    

### B. 상세 실험 결과 표 (2p)

- 16개 모델 조합의 전체 성능 지표
    
- 민원 유형별 세부 성능
    

### C. 평가 데이터셋 예시 (1p)

- 대표 질의-응답 샘플 20개
    

### D. 하이퍼파라미터 튜닝 과정 (1p)

- Grid Search 결과
    
- 최적 파라미터 선정 근거
    

### E. 코드 및 데이터 공개 정보 (0.5p)

- **GitHub**: [github.com/yourname/dasan-graphrag](https://www.google.com/search?q=https://github.com/yourname/dasan-graphrag)
    
- **데이터셋**: Hugging Face Hub (개인정보 제거 버전)


# 제1장 서론

## 1.1 연구 배경 및 필요성


### 1.1.1 공공부문 AI 도입의 가속화와 주권 딜레마
인공지능(AI) 기술의 공공 행정 도입은 더 이상 미래의 비전이 아닌 현재의 운영 현실이 되었다. 전 세계적으로 정부는 데이터 기반 정책 수립, 행정 효율성 증대, 대시민 서비스 품질 향상을 목표로 AI 도입을 전략적으로 가속화하고 있다. 2024년 기준, 전 세계 조직의 78%가 AI를 적극적으로 활용하고 있으며, 이는 전년도의 55%에서 비약적으로 증가한 수치이다 (Stanford University, "2025 AI Index Report", 2025). 정부 및 공공 서비스 분야의 전 세계 AI 시장은 2024년 224억 달러에서 연평균 17.8% 성장하여 2033년 981억 달러에 이를 것으로 전망된다 (Grand View Research, "AI in Government and Public Services Market Report", 2025).

대한민국은 이러한 변화의 선두에 서 있다. 2024년 401개 공공기관을 대상으로 한 조사에서 60.6%가 이미 AI 기술을 도입했으며, AI 관련 공공 조달 계약 총액은 2014년 2,823억 원에서 2023년 1조 3,279억 원으로 10년간 약 4.7배 급증했다 (소프트웨어정책연구소, '2024년 공공부문 AI 도입현황 연구', 2025). 이러한 흐름의 중심에는 정부의 강력한 '클라우드 전환 정책'이 있다. 행정안전부는 2025년까지 모든 행정·공공기관 정보시스템의 클라우드 전환율 100% 달성을 목표로, 전체 시스템의 약 46%를 민간 클라우드로 전환할 계획을 발표했다 (행정안전부, '행정·공공기관 정보자원 클라우드 전환·통합 추진계획', 2021).   

그러나 이 과정에서 공공기관들은 '주권 딜레마(Sovereignty Dilemma)'에 직면한다. 클라우드 기술이 제공하는 신속성과 유연성을 수용하는 동시에, 비용 통제, 데이터 주권, 보안이라는 핵심 가치를 어떻게 지킬 것인가의 문제이다. 민간 시장과 달리 공공 클라우드 시장은 국내 클라우드 서비스 제공사(CSP)가 강세를 보이고 있지만 (컴퓨터월드, '국내 CSP 동향', 2025) , 이는 딜레마를 완전히 해소하지 못한다. 문제의 본질은 CSP의 국적을 넘어, 클라우드라는 기술 모델 자체가 가진 구조적 한계에 있기 때문이다. 이에 대한 대안으로, 최근 한국은행, 국회도서관, 한국전력 등 국가 핵심 기관들은 데이터 보안과 기술 자립을 위해 자체 서버에 직접 AI를 구축하는 온프레미스(On-premise) 방식을 적극적으로 채택 및 검토하고 있다 (한국IDC, '국내 생성형 AI 업무 적용 사례 연구', 2024). 이는 온프레미스 방식이 공공 부문의 주권 딜레마를 해결할 핵심 전략으로 부상하고 있음을 시사한다.   

### 1.1.2 클라우드 API 의존의 구조적 문제
클라우드 기반 AI 서비스, 특히 상용 API 모델은 공공 부문이 장기적으로 채택하기 어려운 세 가지 구조적 문제를 내포하고 있다.

첫째, 경제적 지속 불가능성이다. 상용 API는 대부분 처리하는 텍스트의 양(토큰)에 따라 비용을 지불하는 종량제(Pay-as-you-go) 모델을 채택하고 있다 (OpenAI, "API Pricing", 2025; Togai, "OpenAI API Pricing Model Cost Comparison", 2025). 이는 초기 도입 비용이 낮다는 장점이 있지만, 서울 120다산콜센터와 같이 하루 수만 건의 민원을 처리하는 대용량 서비스의 경우 , 사용량이 많아질수록 운영 비용(OPEX)이 예측 불가능하게 급증하여 공공기관의 장기적인 예산 수립에 큰 부담으로 작용한다 (Redress Compliance, "OpenAI Pricing Models Explained", 2025; Moesif, "Calculating and Reporting COGS for Your OpenAI-Powered API", 2025).   

둘째, 데이터 주권 침해 위험이다. 민감한 시민 데이터를 외부 클라우드에 저장할 경우, 데이터 통제권 상실의 위험에 직면한다. 특히 미국 '데이터의 합법적 해외 사용 명확화법(CLOUD Act)'은 데이터 저장 위치와 무관하게 미국 기업이 통제하는 데이터에 자국 법 집행 기관이 접근할 권한을 부여한다 (Wire, "What the CLOUD Act Really Means for EU Data Sovereignty", 2025). 이는 국내법과 상충할 소지가 있으며, 시민의 민감 정보를 외국 사법 관할권 아래에 두는 잠재적 위험을 안고 있다.   

셋째, 보안 취약성 문제이다. 외부 API를 호출하는 AI 에이전트는 환경 변수를 조작하거나 악의적인 데이터를 주입하는 공격에 취약하다는 연구 결과가 보고되고 있다 (Li et al., "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks", 2025; Kumar et al., "Evaluating the Safety of Computer Use Agents", 2025). 내부 작동을 감사할 수 없는 '블랙박스' 형태의 상용 API는 데이터 유출, 피싱 공격 등의 통로가 될 수 있으며, 이는 공공 서비스의 신뢰를 근본적으로 훼손할 수 있는 심각한 보안 부채로 작용한다.   

### 1.1.3 다산콜센터의 AI 혁신 필요성
서울특별시 120다산콜센터는 이러한 도전 과제를 해결하고 AI 기반 혁신을 이뤄내야 하는 대표적인 공공 서비스 기관이다. 하루 평균 약 22,000건의 방대한 시민 상호작용을 처리하는 다산콜센터는 , AI 기술을 활용한 서비스 고도화에 적극적으로 나서고 있다.   

특히 2025년까지 '스마트 상담센터' 구축을 목표로 하는 "다산콜 2.0" 이니셔티브는 주목할 만하다. 이 계획에 따라 이미 '보이는 ARS'와 STT(음성-텍스트 변환) 시스템을 도입했으며 , 카카오톡 기반 챗봇 서비스인 '서울톡'은 2024년 상반기 기준 전체 상담 건수의 16.7%를 처리하는 등 가시적인 성과를 거두고 있다 (서울시 '내 손안에 서울', 2024). '다산콜 2.0'의 궁극적인 비전은 **"단순·반복 문의는 AI가, 심층적이고 고차원적인 상담은 상담사가 처리"**하는 효율적인 역할 분담 체계를 확립하는 것이다 (보안뉴스, '120다산콜재단 KCCM 컨퍼런스', 2024; 서울기술연구원 보도자료, 2023).   

하지만 현재의 챗봇과 ARS 시스템은 정해진 시나리오 기반의 단순 정보 안내에 머물러 있어, 여러 정보의 종합과 추론이 필요한 복합 민원을 해결하는 데에는 명백한 한계가 있다. '다산콜 2.0'의 비전을 실현하기 위해서는 현재의 시스템을 뛰어넘어, 복잡한 질의의 맥락을 이해하고 다단계 추론을 통해 해결책을 제시할 수 있는 차세대 AI 기술이 필수적이다. 그러나 앞서 분석한 클라우드 API 모델의 구조적 문제들은 이러한 고도화의 걸림돌이 된다. 따라서 본 연구는 다산콜센터가 직면한 기술적, 정책적 과제를 해결하기 위한 대안으로, 비용 효율적이고 안전하며 완벽한 데이터 주권을 보장하는 온프레미스 기반의 경량화 AI 모델 도입의 필요성을 제기한다.
## ## 1.2 연구 목적

### 1.2.1 주 목적: 온프레미스 기반 RAG 스택의 공공 도메인에서의 실용성 실증

본 연구의 **주 목적**은 온프레미스 환경(NVIDIA RTX 3090)에서 **SOTA 모델로 구축한 고품질 지식 그래프와 오픈소스 LLM을 결합한 하이브리드 RAG 시스템이 비용 효율적 상용 클라우드 API(GPT-4.1-mini, Gemini 2.5 Flash)에 근접하거나 동등한 성능을 달성할 수 있음을 실증**하는 데 있다. 이는 최첨단 AI 성능 확보가 곧 해외 빅테크 기업에 대한 기술적·재정적 의존을 통해서만 가능하다는 통념을 깨고, 기술 및 데이터 주권 확보와 고성능 AI 서비스 도입이 양립 가능함을 증명하는 것을 목표로 한다.

핵심 전략은 **"복잡한 일은 최고 모델로, 반복적 일은 효율적 모델로"**라는 원칙을 구현하는 것이다. GPT-5를 활용하여 민원 데이터로부터 도메인 특화 지식 그래프를 일회성으로 구축한 후, 이를 기반으로 오픈소스 LLM을 활용한 Graph RAG 시스템을 구현한다.  이를 통해 본 연구는 공공기관이 데이터 주권과 비용 통제권을 유지하면서도, 시민들에게 최상의 AI 기반 서비스를 제공할 수 있는 기술적 청사진을 제시하고자 한다.

### 1.2.2 세부 목표

본 연구는 다음 네 가지 세부 목표를 설정한다:

**첫째, SOTA 모델 기반 고품질 지식 그래프 자동 구축 방법론을 확립한다.** 최신 SOTA 모델인 GPT-5를 활용하여 민원 데이터로부터 도메인 특화 지식 그래프를 일회성으로 구축한다. 기존 Graph RAG 연구들은 사전 구축된 지식 그래프나 수동으로 정제된 데이터에 의존하는 경우가 많았으나^[Microsoft의 Graph RAG 프레임워크 문서 참조], 본 연구는 원시(raw) 민원 데이터로부터 자동화된 파이프라인을 제시한다. GPT-5의 뛰어난 컨텍스트 이해와 추론 능력을 활용하여 개체(entity) 및 관계(relation)를 정밀하게 추출하고, Neo4j 기반 그래프 데이터베이스로 구축한다. 일회성 작업이므로 상용 API 비용(예상 $200-500)이 허용 가능하며, 구축된 고품질 KG는 이후 RAG 시스템의 영구적 자산이 된다. 또한 오픈소스 LLM으로 동일한 KG 구축을 시도하여 품질 차이를 정량화함으로써, KG 구축에 SOTA 모델이 필요한 이유를 입증한다.

**둘째, 계층적 RAG 모델의 성능 비교를 통해 지식 그래프의 기여도를 정량화한다.** 전통적 검색 증강 생성(RAG)은 사용자 질의와의 의미적 유사성에 기반하여 분리된 텍스트 조각을 검색하는데, 이는 정보 간 구조적 관계를 무시하는 한계가 있다^[Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", 2020]. 고객 서비스 분야 연구에서 표준 RAG 솔루션은 문제의 10-20%만 해결하는 것으로 나타났으며^[Gartner, "How to Improve RAG Performance", 2024], "중간 분실(lost in the middle)" 현상으로 정보 손실이 발생한다^[Liu et al., "Lost in the Middle", 2023].

Graph RAG는 지식 그래프의 구조를 활용하여 개체를 노드로, 관계를 엣지로 표현함으로써 이러한 문제를 해결한다^[Edge et al., "From Local to Global: A Graph RAG Approach", 2024]. 상호 연결된 개체를 검색하고 관계를 탐색할 수 있어 다중 홉 추론(multi-hop reasoning)이 가능하며^[Yang et al., "HotpotQA", 2018], 의료 분야에서 진단 오류를 30% 감소시킨 실적이 있다^[Jin et al., "Biomedical Knowledge Graph RAG", 2024]. 본 연구는 (1) Baseline (RAG 없음), (2) Naive RAG (벡터 유사도 검색), (3) Structured RAG (메타데이터 필터링), (4) Graph RAG (GPT-5 구축 KG 활용)의 4단계 계층적 접근법을 비교하여 각 단계의 성능 향상을 정량화하고, 특히 **고품질 KG가 RAG 성능에 미치는 결정적 기여**를 입증한다.

**셋째, 한국어 공공 민원 도메인의 실무적 과제 해결 능력을 검증한다.** 공공 민원 데이터는 세 가지 실무적 난제를 제시한다: (1) **언어 스타일 이질성** - 공문서체("민원 접수 후 7일 이내 회신")와 구어체("언제쯤 답변받을수있나요")가 동일 대화 내 혼재하여 의도 파악을 어렵게 한다. (2) **도메인 특화 용어 처리** - 행정 전문 용어(전입신고, 주민등록초본)와 시민 일상어("지원금", "건") 간 매핑이 필요하며, 이는 일반 LLM의 학습 데이터에 부족하다. (3) **불완전 정보에 대한 강건성** - 필수 정보 누락, 맞춤법 오류, 외부 참조(첨부파일, URL) 등 비정형 입력을 처리해야 한다.

본 연구는 4개 오픈소스 LLM(EXAONE-3.5 7.8B/32B, Qwen 3-8B, Gemma 3-12B, GPT-OSS-20B)의 양자화 버전을 비교하여, 이러한 도메인 특화 과제에 대한 모델별 강건성을 검증한다. 특히 LG AI Research의 EXAONE-3.5 모델이 Ko-LongRAG 벤치마크에서 최고 성능(32B: 85.3점, 2.4B: 74.7점)을 기록했다는 점^[Kim et al., "Ko-LongRAG Benchmark", 2024]을 감안하여, 한국어 RAG 시스템에서의 우위를 재검증한다. 또한 고품질 지식 그래프의 동의어 네트워크와 표준화된 개체 표현이 이러한 실무적 과제를 완화하는 효과를 측정한다.

**넷째, 하이브리드 전략의 총소유비용(TCO) 우위를 입증한다.** 오픈소스 모델의 자체 호스팅은 하드웨어 구매의 초기 자본지출(CAPEX)을 요구하지만, 상용 API의 가변적 토큰당 운영비용(OPEX)을 제거한다^[AWS, "Cost Optimization for LLM Inference", 2024]. 24GB VRAM의 RTX 3090은 고성능과 가용성, 비용의 균형을 맞춘 현실적 하드웨어 목표이며, 양자화 기술(4bit/8bit)을 통해 최대 32B 파라미터 모델까지 구동 가능하다^[Dettmers et al., "QLoRA: Efficient Finetuning of Quantized LLMs", 2023].

본 연구는 다음과 같은 비용 구조를 분석한다: (1) **초기 투자** - 하드웨어($2,000) + GPT-5 KG 구축($200-500) + 개발($3,000), (2) **연간 운영** - 온프레미스 전력비($600) vs. GPT-4.1-mini API($2,400) vs. Gemini 2.5 Flash API($1,200). 일 22,000건 처리 기준으로 하이브리드 전략은 14-18개월 내 투자를 회수하며, 5년 TCO는 $8,200로 순수 상용 API 대비 최대 36% 절감된다. 특히 KG 구축 비용($200-500)은 전체 TCO의 3-6%에 불과하여, 일회성 고품질 투자의 정당성을 입증한다. 이를 통해 지방자치단체 예산 범위($5,000 이내)에서 복제 가능한 실용적 배포 경로를 제시한다.

## 1.3 연구 내용 및 범위

### 1.3.1 비교 대상 및 실험 설계

본 연구는 **하이브리드 RAG 시스템(SOTA 모델 구축 KG + 오픈소스 LLM 운영)**의 성능 및 비용 우위를 검증한다.

**Phase 1: 지식 그래프 구축**에서 GPT-5를 활용하여 민원 데이터로부터 개체(민원인, 부서, 서비스명, 법령)와 관계(절차적·조건적·참조적)를 추출하여 Neo4j 그래프 데이터베이스로 구축한다. 오픈소스 LLM(EXAONE-3.5 32B, Qwen 3-8B)으로 동일 작업을 수행하여 품질을 비교한다.

**Phase 2: RAG 시스템 평가**에서 5개 오픈소스 LLM의 INT4 양자화 버전을 평가한다. EXAONE-3.5 7.8B/32B는 Ko-LongRAG 최고 성능(85.3점)으로 한국어에 최적화되어 있으며, Qwen 3-8B는 131K 컨텍스트로 긴 문서에 강점이 있다. Gemma 3-12B는 140개 언어를 지원하고, GPT-OSS-20B는 MoE 아키텍처로 구조화 쿼리에 특화되어 있다.

4단계 RAG 방식과 조합하여 총 20개 구성을 평가한다: ① Baseline (RAG 없음), ② Naive RAG (벡터 검색), ③ Structured RAG (메타데이터 필터링), ④ Graph RAG (다중 홉 추론). 비교 기준은 GPT-4.1-mini ($0.15/$0.60 per 1M tokens) 및 Gemini 2.5 Flash ($0.075/$0.30 per 1M tokens)이다.

### 1.3.2 평가 방법론

**정량 평가**는 표준 NLP 메트릭(BLEU, ROUGE-L, BERTScore)과 RAGAS 프레임워크의 RAG 지표(Context Relevance, Answer Relevance, Faithfulness, Correctness)를 사용한다. **정성 평가**는 다단계 추론 능력, 언어 스타일 이질성 처리, 불완전 정보 대응을 민원 전문가 3인이 5점 척도로 평가한다. **비용 분석**은 3년 TCO를 비교한다. 하이브리드 전략은 초기 투자(HW $2,000 + KG 구축 + 개발 $3,000)와 연간 운영비(전력)로 구성되며, 상용 API는 초기 $500에 일 22,000건 기준 월 $100-200의 지속 비용이 발생한다.

### 1.3.3 실험 환경

온프레미스 워크스테이션 사양은 다음과 같다:

|구분|사양|
|---|---|
|**GPU**|NVIDIA GeForce RTX 3090 Ti (24GB VRAM)|
|**CPU**|Intel Core i5-11400 (6코어/12스레드)|
|**RAM**|32GB DDR4 3200MHz|
|**스토리지**|Samsung 980 NVMe 500GB + HDD 2TB|

총 $2,000-2,500 수준으로 지자체 도입 가능한 사양이다. 소프트웨어는 Neo4j (그래프 DB), LangChain (RAG), vLLM (추론), bitsandbytes (양자화)를 사용한다. 데이터는 120다산콜센터 민원 기록(익명화)에서 복지·교통·건축·주민등록 도메인의 테스트셋 1,000건을 추출한다.

## 1.4 연구의 차별점과 기여도

### 1.4.1 학술적 기여

**첫째, 계층적 RAG 설계를 통한 구성요소별 기여도 정량화**를 제공한다. 기존 연구들은 Graph RAG의 전체적 성능 우위를 입증했으나[23][22], 각 구성요소(메타데이터 필터링, 그래프 구조화, 다중 홉 추론)의 개별 기여도를 분리하여 측정하지 못했다. 본 연구는 4단계 계층적 접근을 통해 각 기술 요소가 성능에 미치는 영향을 정량적으로 분석한다. 2024-2025년 Graph RAG 연구의 급증[40]과 도메인 특화 변종(MedRAG, PathRAG, LightRAG 등) 개발[40][41]은 이 분야가 개념 증명에서 정교한 엔지니어링으로 이동하고 있음을 보여주며, 본 연구는 이러한 첨단 연구 흐름에 기여한다.

**둘째, 한국어 민원 도메인에 특화된 평가 체계**를 제시한다. RAG 애플리케이션 시장이 2030년까지 110억 달러로 성장할 것으로 예상되지만[42], 기존 연구는 주로 영어권 일반 도메인에 집중되어 있다. 한국 공공 민원의 언어적 복잡성—교착어 구조, 공공언어와 구어체의 혼재, 한자어 비중—을 체계적으로 다루는 연구는 부재하다. 본 연구는 이러한 학술적 공백을 메우며, 비영어권 공공 도메인 AI 연구의 방법론적 기준을 제시한다.

### 1.4.2 실무적 기여

**첫째, 공공기관 도입 가능한 비용 효율적 솔루션**을 제시한다. 상용 API 의존 모델의 경제적 지속 불가능성과 데이터 주권 위험을 해결하는 실용적 대안을 제공한다. <표 1>은 두 접근법의 차이를 요약한다.

**<표 1> 상용 클라우드 API vs. 온프레미스 오픈소스 시스템 비교**

| 기능      | 상용 클라우드 API        | 온프레미스 오픈소스         |
| ------- | ------------------ | ------------------ |
| 비용 구조   | 토큰당 과금, 영구적 OPEX   | 초기 CAPEX 후 낮은 OPEX |
| 데이터 주권  | 심각한 위험 (CLOUD Act) | 완전한 주권 확보          |
| 보안      | 외부 공격 벡터 노출        | 방화벽 내 격리           |
| 맞춤화     | 블랙박스, 제한적          | 완전한 접근 및 맞춤화       |
| 공급업체 종속 | 높음                 | 낮음                 |

**둘째, 오픈소스 기반 AI 기술 주권 확보 방안**을 제시한다. 최고 수준 오픈소스 LLM의 성능 동등성과 한국어 특화 모델(EXAONE Deep)의 우수성을 실증함으로써, 공공 AI 배포에서 디지털 자율성 달성의 실행 가능성을 입증한다. 이는 지방 자치 단체 및 지역 공공 기관이 현실적 자원으로 주권 AI를 구현할 수 있는 복제 가능한 청사진을 제공한다.

## 1.5 논문 구성

본 논문은 다음과 같이 구성된다. 제2장에서는 RAG 기술과 Graph RAG의 발전, 오픈소스 LLM 생태계, 한국어 NLP 과제에 관한 선행연구를 검토한다. 제3장에서는 계층적 RAG 시스템 아키텍처와 실험 설계를 상세히 기술한다. 제4장에서는 16개 조합에 대한 정량·정성 평가 결과와 비용 분석을 제시한다. 제5장에서는 연구 결과의 의의와 한계, 향후 연구 방향을 논의하며 결론을 맺는다.

	# **제2장 이론적 배경 및 관련 연구**
	
	## **2.1 기술적 배경**
	
	### **2.1.1 인공지능과 자연어처리의 발전**
	
	**인공지능은 규칙 기반 시스템에서 통계 기반 기계학습, 딥러닝을 거쳐 현재의 대규모 언어모델(LLM) 시대로 진화해왔다. 특히 2017년 Vaswani 등이 제안한 Transformer 아키텍처는 자연어처리 분야에 혁명적 변화를 가져왔다.**
	
	**Transformer의 핵심 혁신**
	
	**Transformer의 핵심은 Self-Attention 메커니즘이다. 이는 입력 시퀀스의 각 위치가 다른 모든 위치와의 관계를 동시에 계산하여, 기존 RNN의 순차적 처리 한계를 극복하고 병렬 처리를 가능하게 했다.**
	
	**Self-Attention의 계산 과정:**
	
	```
	Attention(Q, K, V) = softmax(QK^T / √d_k)V
	```
	
	**여기서 Q(Query), K(Key), V(Value)는 입력의 선형 변환이며, d_k는 Key 벡터의 차원이다.**
	
	### **2.1.2 대규모 언어모델(LLM)의 진화와 한계**
	
	**LLM의 발전**
	
	**2018년 BERT의 등장 이후, GPT 시리즈, T5 등 사전학습-미세조정(pre-training & fine-tuning) 패러다임이 확립되었다. 모델 크기가 수십억 파라미터로 확대되면서 in-context learning, chain-of-thought 추론 등 창발적 능력(emergent abilities)이 나타났다.**
	
	**LLM의 근본적 한계**
	
	**LLM은 강력하지만 세 가지 핵심 한계를 가진다:**
	1. **환각(Hallucination): 사실과 다른 정보를 그럴듯하게 생성**
	2. **지식 업데이트 어려움: 학습 시점 이후 정보 반영 불가**
	3. **도메인 특화 부족: 특정 분야의 전문 지식 부족**
	
	**이러한 한계는 LLM을 실무에 적용하는 데 있어 신뢰성 문제를 야기하며, 특히 공공 서비스처럼 정확성이 필수적인 분야에서는 치명적이다.**
	
	### **2.1.3 Retrieval-Augmented Generation (RAG) 기술**
	
	**RAG의 등장 배경**
	
	**Lewis 등(2020)이 제안한 RAG는 LLM의 파라미터적 지식(parametric knowledge)과 외부 검색을 통한 비파라미터적 지식(non-parametric knowledge)을 결합한 혁신적 패러다임이다. 이는 LLM의 환각 문제를 완화하고 최신 정보를 제공할 수 있는 핵심 솔루션으로 부상했다.**
	
	**Basic RAG 아키텍처**
	
	**RAG 시스템은 세 단계로 구성된다:**
	
	4. **인덱싱(Indexing): 문서를 작은 청크(chunk)로 분할하고 벡터 임베딩 생성**
	5. **검색(Retrieval): 질의와 유사한 청크를 벡터 유사도 검색으로 탐색**
	6. **생성(Generation): 검색된 맥락과 질의를 결합하여 LLM이 답변 생성**
	
	**벡터 검색의 원리**
	
	**텍스트를 고차원 벡터 공간에 매핑하여, 코사인 유사도(cosine similarity)를 통해 의미적으로 유사한 문서를 찾는다:**
	
	```
	similarity(q, d) = (q · d) / (||q|| × ||d||)
	```
	
	**Advanced RAG 기법**
	
	**기본 RAG의 한계를 극복하기 위한 발전된 기법들:**
	
	- **Hybrid Search: 벡터 검색과 키워드 기반 검색(BM25) 결합**
	- **Re-ranking: 검색된 문서를 재정렬하여 관련성 향상**
	- **Query Rewriting: 질의를 재구성하여 검색 품질 개선**
	- **HyDE: 가상의 답변을 생성하여 검색에 활용**
	
	**RAG는 LLM의 한계를 극복하는 현실적이고 효과적인 솔루션으로, 특히 도메인 특화 AI 시스템 구축에 필수적인 기술이다.**
	
	### **2.1.4 Graph RAG 및 최신 변형 기술**
	
	**Graph RAG의 동기**
	
	**기존 RAG는 문서를 평면적인 청크로 취급하여 다음의 한계를 가진다:**
	1. **문맥 단편화: 청크 분할로 인한 맥락 손실**
	2. **다중 홉 추론 실패: 여러 정보를 연결하는 질문에 취약**
	3. **전역적 이해 부족: 데이터셋 전체에 대한 종합적 질문 처리 어려움**
	
	**Graph RAG 아키텍처 (Microsoft, 2024)**
	
	**Edge 등(2024)이 제안한 Graph RAG는 지식을 그래프 구조로 표현하여 근본적 개선을 달성했다.**
	
	**핵심 구성 요소:**
	
	4. **지식 그래프 구축**
	    - **LLM을 활용한 개체(Entity) 및 관계(Relationship) 추출**
	    - **(주어, 술어, 목적어) 형태의 삼중항(triple) 생성**
	    - **개체 정규화 및 일관성 확보**
	5. **Community Detection**
	    - **Leiden 알고리즘 등을 통해 연관 개체 클러스터 발견**
	    - **각 커뮤니티에 대한 요약(summary) 생성**
	6. **검색 전략**
	    - **Local Search: 특정 개체 중심의 이웃 탐색**
	    - **Global Search: 커뮤니티 요약 기반 전체론적 답변**
	
	**PathRAG와 LightRAG**
	
	**최신 변형 기술들은 효율성과 성능을 더욱 개선했다:**
	
	- **PathRAG (2024):**
	    - **질의 관련 개체 간 핵심 경로(path)만 추출**
	    - **중복 정보 제거로 토큰 소비 30% 감소**
	    - **그래프 순회 최적화를 통한 검색 속도 향상**
	- **LightRAG (2024):**
	    - **경량화된 그래프 인덱싱 구조**
	    - **실시간 그래프 업데이트 지원**
	    - **더 적은 메모리로 대규모 그래프 처리**
	
	**기술적 우위성**
	
	**Graph RAG는 Naive RAG 대비 다음의 개선을 보인다:**
	
	- **정확도: 복잡한 다중 홉 질의에서 15-30% 성능 향상**
	- **사실성: 그래프의 명시적 관계로 환각 현상 감소**
	- **설명 가능성: 추론 경로 추적 가능**
	- **일관성: 구조화된 지식으로 모순 방지**
	
	**특히 Diffbot의 벤치마크에서 스키마 의존 질의에 대해 벡터 RAG가 0% 정확도를 기록한 반면, Graph RAG는 높은 성능을 달성했다.**
	
	**지식 그래프 구축의 자동화**
	
	**LLM 기반 KGC 파이프라인**
	
	**전통적으로 지식 그래프 구축(Knowledge Graph Construction, KGC)은 수작업에 의존했으나, LLM의 등장으로 자동화가 가능해졌다.**
	
	**핵심 단계:**
	1. **텍스트 청킹: 문서를 LLM 컨텍스트 윈도우에 맞게 분할**
	2. **개체-관계 추출: LLM 프롬프트로 삼중항 추출**
	3. **개체 표준화: "AI", "A.I.", "인공지능"을 단일 노드로 통합**
	4. **관계 추론: 명시되지 않은 암묵적 관계 발견**
	
	**프롬프트 엔지니어링**
	
	**효과적인 KGC를 위한 프롬프트 설계 원칙:**
	
	```
	# 예시 프롬프트 구조
	당신은 지식 그래프 전문가입니다.
	다음 텍스트에서 개체와 관계를 추출하여 JSON 형식으로 출력하세요.
	
	규칙:
	1. 개체 이름은 일관되게 유지
	2. 관계는 1-3 단어로 간결하게
	3. 대명사는 실제 개체로 대체
	
	출력 형식:
	{
	  "entities": [...],
	  "relationships": [
	    {"subject": "개체1", "predicate": "관계", "object": "개체2"}
	  ]
	}
	```
	
	**EDC 프레임워크 (EMNLP 2024)**
	
	**"Extract, Define, Canonicalize" 프레임워크는 대규모 스키마 처리 문제를 해결했다:**
	
	4. **Extract: 스키마 없이 자유롭게 정보 추출**
	5. **Define: 추출된 요소에 대해 LLM이 정의 생성**
	6. **Canonicalize: 의미적으로 동일한 요소 통합**
	
	**이 접근법은 기존 방식의 컨텍스트 창 한계를 극복하며 확장성을 크게 개선했다.**
	
	**도전 과제**
	
	**자동화된 KGC의 주요 과제:**
	
	- **환각 제어: LLM이 존재하지 않는 관계 생성 가능**
	- **확장성: 대규모 문서 처리 시 API 비용 증가**
	- **품질 평가: 생성된 그래프 품질 측정 기준 부족**
	
	**이러한 과제에도 불구하고, LLM 기반 KGC는 지식 관리의 민첩성을 혁신적으로 향상시키며, 도메인 특화 Graph RAG 시스템 구축의 핵심 기술로 자리잡고 있다.**
	
	**---**
	
	## **2.2 관련 연구**
	
	### **2.2.1 RAG 시스템 성능 비교 연구**
	
	**국외 연구**
	
	**Edge 등(2024)의 Microsoft GraphRAG 연구는 체계적인 비교 실험을 수행했다:**
	
	- **데이터셋: 뉴스 기사, 팟캐스트 등 다양한 도메인**
	- **평가 지표: Comprehensiveness, Diversity, Empowerment**
	- **결과: 전역 질의에서 Naive RAG 대비 30-40% 성능 향상**
	
	**PathRAG(2024)는 효율성에 초점을 맞췄다:**
	
	- **토큰 소비 30% 감소하며 동등한 정확도 달성**
	- **다중 홉 추론에서 구조화된 경로의 중요성 입증**
	
	**국내 연구**
	
	**한국어 RAG 연구는 초기 단계이나 주목할 만한 진전:**
	
	- **LG AI Research의 EXAONE 모델 기반 RAG 시스템**
	- **네이버의 한국어 임베딩 모델(KoBERT, KR-SBERT) 개발**
	- **한국전자통신연구원(ETRI)의 도메인 특화 RAG 연구**
	
	**연구 공백**
	
	**공공 민원 도메인에 특화된 RAG 연구는 거의 부재하다. 기존 연구들은 주로:**
	
	- **일반 질의응답에 초점**
	- **영어 중심 평가**
	- **상용 API 기반 실험**
	
	**본 연구는 이러한 공백을 메우며, 온프레미스 환경에서 한국어 민원 데이터에 최적화된 Graph RAG 시스템을 실증적으로 평가한다.**
	
	### **2.2.2 공공부문 AI 챗봇 연구 동향**
	
	**국내 공공 챗봇 현황**
	
	**행정안전부(2024) 보고서에 따르면:**
	
	- **중앙부처 및 지방자치단체의 80% 이상이 AI 챗봇 운영 중**
	- **연간 운영 비용 평균 5천만원 이상 (상용 API 의존)**
	- **사용자 만족도는 60-70% 수준으로 개선 필요**
	
	**주요 문제점**
	
	1. **상용 API 의존**
	    - **OpenAI, Anthropic 등 해외 서비스에 종속**
	    - **데이터 주권 및 보안 리스크**
	    - **높은 운영 비용 (월 수백만원)**
	2. **고비용 구조**
	    - **토큰당 과금 방식으로 대규모 서비스 제약**
	    - **예산 한계로 인한 기능 제한**
	3. **도메인 커스터마이징 어려움**
	    - **범용 LLM의 행정 용어 이해 부족**
	    - **민원 맥락 파악 미흡**
	
	**본 연구의 온프레미스 오픈소스 접근은 이러한 문제들에 대한 실질적 해결책을 제시한다.**
	
	### **2.2.3 한국어 특화 LLM 및 RAG 연구**
	
	**국산 LLM 현황**
	
	- **EXAONE (LG AI Research): 한국어 및 코드 생성에 강점**
	- **Polyglot-Ko (EleutherAI): 한국어 오픈소스 LLM**
	- **HyperCLOVA (Naver): 대규모 한국어 데이터 학습 (비공개)**
	
	**한국어 임베딩 모델**
	
	**벡터 검색을 위한 한국어 임베딩 모델 발전:**
	
	- **KoBERT, KR-SBERT: 초기 한국어 특화 모델**
	- **jhgan/ko-sroberta-multitask: 현재 가장 널리 사용**
	- **intfloat/multilingual-e5-large: 다국어 지원 고성능 모델**
	
	**이러한 모델들은 한국어 RAG 시스템의 핵심 기반 기술이다.**
	
	**---**
	
	## **2.3 연구 대상 도메인 분석**
	
	### **2.3.1 다산콜센터 운영 현황**
	
	**서비스 개요**
	
	**서울시 120 다산콜센터는 서울시민의 종합 민원 상담 플랫폼이다.**
	
	**운영 규모 (2024년 기준)**
	
	- **일평균 민원 처리: 4,000건 이상**
	- **상담원: 65명**
	- **연간 처리 건수: 약 140만 건**
	- **상담 분야: 120개 이상 (행정, 복지, 교통, 환경 등)**
	
	**AI 도입 필요성**
	
	1. **반복 질의 자동화: 전체 민원의 약 40%가 반복적 질문**
	2. **상담 품질 향상: 정확하고 일관된 정보 제공**
	3. **상담원 업무 경감: 복잡한 민원에 집중 가능**
	4. **24시간 서비스: 야간/주말 자동 응답 지원**
	5. **비용 효율성: 인력 증원 없이 서비스 확대**
	
	### **2.3.2 민원 데이터의 특수성 및 도전과제**
	
	**데이터 특수성**
	
	**민원 데이터는 일반 텍스트와 다른 고유한 특성을 가진다:**
	
	6. **구어체 표현**
	    
	    - **"애기 낳았는데 지원금은요?"**
	    - **"쓰레기 버리는 날 언제죠?"**
	7. **감정 표현 포함**
	    
	    - **불만, 긴급성, 혼란 등 감정 상태 혼재**
	    - **감정을 고려한 응답 필요**
	8. **행정 전문 용어**
	    
	    - **"기초생활수급자", "근로장려금", "재개발구역"**
	    - **일반 LLM이 정확히 이해하기 어려운 용어**
	9. **정책 간 연관성**
	    
	    - **복지 정책들의 복잡한 자격 조건 관계**
	    - **다중 홉 추론 필요**
	
	**도전 과제**
	
	10. **데이터 비정형성**
	    
	    - **동일한 정책을 다양한 표현으로 질문**
	    - **맞춤법 오류, 은어 사용**
	11. **빈번한 정책 변경**
	    
	    - **분기별, 연도별 정책 업데이트**
	    - **실시간 지식 동기화 필요**
	12. **개인정보 처리**
	    
	    - **민감 정보 마스킹**
	    - **데이터 익명화 및 보안**
	
	**본 연구의 접근**
	
	**이러한 특수성을 해결하기 위해:**
	
	13. **도메인 특화 Graph RAG**
	    
	    - **행정 용어 온톨로지 구축**
	    - **정책 간 관계 그래프화**
	14. **한국어 최적화**
	    
	    - **한국어 특화 LLM 및 임베딩 모델 선정**
	    - **구어체 처리 강화**
	15. **업데이트 메커니즘**
	    
	    - **증분 학습(Incremental Learning) 구조**
	    - **정책 변경 시 그래프 부분 업데이트**
	
	**---**
	
	## **2.4 오픈소스 LLM 및 프레임워크 동향**
	
	### **2.4.1 연구 대상 모델 특성 분석**
	
	**본 연구는 RTX 3090Ti(24GB VRAM) 환경에서 구동 가능한 4개 오픈소스 모델을 선정했다.**
	
	**1. gpt-oss-20b (OpenAI, 2025)**
	
	- **아키텍처: Mixture-of-Experts (MoE)**
	- **파라미터: 20B (활성 파라미터 약 6B)**
	- **특징:**
	    - **Tool Use 및 CoT 추론 강화**
	    - **복잡한 워크플로우 처리 우수**
	    - **o3-mini 수준의 성능 목표**
	- **강점: 추론 능력, 도구 사용**
	- **약점: 모델 크기로 인한 추론 속도**
	
	**2. Gemma 3 12B (Google, 2025)**
	
	- **라이선스: Apache 2.0 (상업적 자유)**
	- **특징:**
	    - **다국어 처리 능력 강화**
	    - **논리적 추론 성능 개선**
	    - **메모리 효율적 설계**
	- **강점: 균형잡힌 성능, 상업적 활용 자유**
	- **약점: 한국어 특화 부족**
	
	**3. Qwen 3 8B (Alibaba, 2025)**
	
	- **특징:**
	    - **향상된 다국어 지원**
	    - **긴 컨텍스트 처리 (최대 32K 토큰)**
	    - **컨텍스트 이해 능력 우수**
	- **강점: 복잡한 민원 상담 처리, 효율성**
	- **약점: 중국어 중심 학습**
	
	**4. EXAONE 7.8B (LG AI Research)**
	
	- **특징:**
	    - **한국어 데이터에 특화 학습**
	    - **국내 행정 용어 이해도 높음**
	    - **한국어 문맥 파악 우수**
	- **강점: 한국어 민원 처리 최적화**
	- **약점: 영어 등 다국어 능력 제한**
	
	### **2.4.2 온프레미스 구축 가능성 검토**
	
	**하드웨어 요구사항**
	
	**RTX 3090Ti (24GB VRAM) 환경:**
	
	|모델|양자화|VRAM 사용량|추론 속도|
	|---|---|---|---|
	|gpt-oss-20b|4-bit|~18GB|20 tokens/s|
	|Gemma 3 12B|4-bit|~14GB|35 tokens/s|
	|Qwen 3 8B|8-bit|~16GB|40 tokens/s|
	|EXAONE 7.8B|8-bit|~15GB|42 tokens/s|
	
	**양자화 기술**
	
	- **4-bit 양자화 (GPTQ, AWQ): 메모리 1/4 축소, 성능 5% 내 손실**
	- **8-bit 양자화 (bitsandbytes): 메모리 1/2 축소, 성능 2% 내 손실**
	
	**배포 전략**
	
	1. **단일 GPU 배포: 8B 이하 모델에 적합**
	2. **텐서 병렬화: 12B 이상 모델을 다중 GPU로 분산**
	3. **파이프라인 병렬화: 순차적 레이어 분산 처리**
	
	**비용 분석**
	
	**온프레미스 vs 상용 API (월 100만 요청 기준):**
	
	|항목|온프레미스|GPT-4 API|
	|---|---|---|
	|초기 투자|700만원 (HW)|0원|
	|월 운영비|40만원|500만원|
	|연간 비용|1,180만원|6,000만원|
	|**절감률**|**기준**|**-80.3%**|
	
	**온프레미스 구축 시 1.5개월 만에 손익분기점 도달하며, 데이터 주권 확보라는 부가 가치도 얻는다.**
	
	### **2.4.3 RAG 프레임워크 및 도구**
	
	**LangChain**
	
	- **역할: RAG 파이프라인 오케스트레이션**
	- **기능:**
	    - **문서 로더, 텍스트 분할기**
	    - **벡터 스토어 통합**
	    - **LLM 체인 구성**
	- **장점: 풍부한 에코시스템, 빠른 프로토타이핑**
	
	**LlamaIndex**
	
	- **역할: 데이터 인덱싱 및 검색 최적화**
	- **기능:**
	    - **계층적 인덱스 구조**
	    - **다양한 검색 전략 (벡터, 키워드, 하이브리드)**
	    - **쿼리 변환 엔진**
	- **장점: RAG에 특화된 설계**
	
	**Neo4j (그래프 데이터베이스)**
	
	- **역할: Graph RAG의 지식 그래프 저장소**
	- **기능:**
	    - **네이티브 그래프 스토리지**
	    - **Cypher 쿼리 언어**
	    - **벡터 유사도 검색 지원 (v5.13+)**
	- **장점: 그래프 순회 성능, 확장성**
	
	**통합 아키텍처**
	
	```
	사용자 질의
	    ↓
	LangChain (파이프라인)
	    ↓
	[벡터 검색] → FAISS/Milvus
	    ↓
	[그래프 순회] → Neo4j
	    ↓
	[검색 결과 통합]
	    ↓
	LLM (gpt-oss / Gemma / Qwen / EXAONE)
	    ↓
	생성된 답변
	```
	
	**이러한 도구들의 조합은 효율적이고 확장 가능한 Graph RAG 시스템 구축을 가능하게 한다.**
	
	**---**
	
	## **2.5 소결**
	
	**본 장에서는 Graph RAG 시스템 구축을 위한 이론적 기반을 확립했다. 주요 내용을 요약하면:**
	
	**핵심 기술 스택**
	
	1. **LLM의 한계 인식: 환각, 지식 업데이트 어려움, 도메인 특화 부족**
	2. **RAG의 필요성: 비파라미터적 지식으로 LLM 보완**
	3. **Graph RAG의 우월성: 구조화된 지식으로 15-30% 성능 향상**
	4. **자동화된 KGC: LLM 기반 지식 그래프 구축의 실현 가능성**
	
	**연구의 위치**
	
	- **공공 민원 도메인 Graph RAG 연구의 공백 메우기**
	- **온프레미스 오픈소스 기반 실증 연구**
	- **한국어 특화 및 비용 효율성 검증**
	
	**기술적 기반**
	
	- **4개 오픈소스 LLM의 특성 이해**
	- **RTX 3090Ti 환경에서의 구동 가능성 확인**
	- **LangChain, LlamaIndex, Neo4j를 활용한 통합 프레임워크**
	
	**다음 장에서는 이러한 이론적 배경을 바탕으로 본 연구의 실험 설계 및 방법론을 상세히 기술한다.**

---

## 참고문헌

[1] Stanford University (2025). 2025 AI Index Report. [2] Grand View Research (2025). AI in Government and Public Services Market Report. [3] 소프트웨어정책연구소 (2025). 2024년 공공부문 AI 도입현황 연구. [4] OpenAI (2025). API Pricing. [5] Togai (2025). OpenAI API Pricing Model Cost Comparison. [6] 서울특별시 (2011). 120다산콜센터 운영현황 및 사례분석. [7] 서울특별시 (2023). 120다산콜센터. [8] Redress Compliance (2025). OpenAI Pricing Models Explained. [9] Moesif (2025). Calculating and Reporting COGS for Your OpenAI-Powered API. [10] BytePlus (2025). OpenAI API Pricing. [11] Wire (2025). What the CLOUD Act Really Means for EU Data Sovereignty. [12] Li et al. (2025). Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks. arXiv. [13] Kumar et al. (2025). Evaluating the Safety of Computer Use Agents. arXiv. [14] Greshake et al. (2023). Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. arXiv. [15] Zhang et al. (2024). Agent Security Bench. arXiv. [16] 서울시티 (2024). 120다산콜재단, 2024년 한국산업의 서비스 품질지수(KSQI) 우수 콜센터 선정. [17] 연합뉴스 (2024). 서울시 120다산콜, AI로 '24시간 민원상담' 실시간채팅 추진. [18] 서울특별시의회 (2024). 김혜영 시의원, '120다산콜센터, AI 상담 전환 속 상담사 고용 안정성 확보...'. [19] arXiv (2025). Knowledge Graph-Guided Retrieval Augmented Generation. [20] arXiv (2024). Graph Retrieval-Augmented Generation: A Survey. [21] Forethought (2024). State of AI in CX: Peak Season Insights 2024. [22] Data Science Dojo (2025). Graph RAG vs. RAG: The Next Evolution in AI. [23] NAACL Findings (2025). Graph Retrieval-Augmented Generation. [24] arXiv (2023). GraphRAG: Reasoning on Graphs with Retrieval-Augmented LLMs. [25] arXiv (2025). A Hybrid Graph-Text RAG for Multi-hop Question Answering. [26] Fastcampus (2024). 복잡하고 까다로운 자연어 처리, '한국어'가 유독 어려운 이유. [27] 김기현 (2021). PyTorch로 시작하는 자연어 처리. [28] 국립국어원 (2019). 2019년 중앙행정기관 공공언어 진단 최종 보고서. [29] KT Enterprise (2023). AI 챗봇의 핵심 기술, 자연어 처리(NLP)와 텍스트 분석. [30] DPG (2024). 챗GPT 시대, AI는 인간의 언어를 어떻게 이해할까? [31] Latitude (2025). Open-Source vs. Proprietary LLMs: A Cost Breakdown. [32] Medium (2025). How to give your RTX GPU nearly infinite memory for LLM inference. [33] arXiv (2025). QRazor: A Reliable and Effortless 4-bit Quantization Scheme for LLMs. [34] arXiv (2025). Silver Bullet: A Lightweight Measure-Locate-Restore Loop for Low-Bit LLM Quantization. [35] Danawa DPG (2025). LG AI연구원, 추론 AI의 새 기준 '엑사원 딥' 공개. [36] dev.to (2025). Qwen3-Max (2025) Complete Release Analysis. [37] Alibaba Cloud (2025). Alibaba Cloud Unveils Strategic Roadmaps for the Next-Generation AI Innovations. [38] Google AI (2025). Gemma 3 Model Card. [39] Google Cloud (2025). Generative AI models. [40] GitHub (2025). Awesome-GraphRAG. [41] AAAI (2025). LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph. [42] Grand View Research (2025). Retrieval Augmented Generation (RAG) Market Report.