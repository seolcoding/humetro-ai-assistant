{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다산콜센터 질의-응답 데이터 EDA\n",
    "\n",
    "## 데이터셋 개요\n",
    "- **출처**: AI Hub - 민원(콜센터) 질의-응답 데이터\n",
    "- **도메인**: 다산콜센터 (서울시 120 콜센터)\n",
    "- **데이터 형태**: 질의-응답 대화쌍 + 음성 데이터\n",
    "- **카테고리**: \n",
    "  - 대중교통 안내\n",
    "  - 코로나19 관련 상담\n",
    "  - 일반행정 문의\n",
    "  - 생활하수도 관련 문의\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & 한국어 폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 폰트 설정 (macOS/Windows/Linux 호환)\n",
    "def setup_korean_font():\n",
    "    \"\"\"\n",
    "    한국어 폰트를 자동으로 찾아서 설정합니다.\n",
    "    macOS, Windows, Linux에서 모두 작동합니다.\n",
    "    \"\"\"\n",
    "    # 시스템별 한국어 폰트 후보\n",
    "    font_candidates = [\n",
    "        # macOS\n",
    "        'AppleGothic', 'Apple SD Gothic Neo', 'AppleMyungjo',\n",
    "        # Windows\n",
    "        'Malgun Gothic', 'Gulim', 'Batang', 'Dotum',\n",
    "        # Linux\n",
    "        'NanumGothic', 'NanumBarunGothic', 'UnDotum',\n",
    "        # 기타\n",
    "        'DejaVu Sans'\n",
    "    ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font_name in font_candidates:\n",
    "        if font_name in available_fonts:\n",
    "            plt.rcParams['font.family'] = font_name\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "            print(f\"✓ 한국어 폰트 설정: {font_name}\")\n",
    "            return font_name\n",
    "    \n",
    "    # 폰트를 찾지 못한 경우\n",
    "    print(\"⚠️  한국어 폰트를 찾을 수 없습니다. 기본 폰트를 사용합니다.\")\n",
    "    print(\"   나눔고딕 설치 권장: https://hangeul.naver.com/2017/nanum\")\n",
    "    return None\n",
    "\n",
    "# 폰트 설정 실행\n",
    "korean_font = setup_korean_font()\n",
    "print(f\"\\n사용 가능한 한국어 폰트 목록 (처음 10개):\")\n",
    "korean_fonts = [f.name for f in fm.fontManager.ttflist if 'Korean' in f.name or 'Hangul' in f.name or 'Nanum' in f.name]\n",
    "for font in korean_fonts[:10]:\n",
    "    print(f\"  - {font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_DIR = Path(\"/Users/sdh/Dev/02_production_projects/humetro-ai-assistant\")\n",
    "DATA_DIR = BASE_DIR / \"data/dasan_call/extracted\"\n",
    "OUTPUT_DIR = BASE_DIR / \"data/processed/dasan_eda\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "TRAINING_LABELED = DATA_DIR / \"training/labeled\"\n",
    "TRAINING_SOURCE = DATA_DIR / \"training/source\"\n",
    "VALIDATION_LABELED = DATA_DIR / \"validation/labeled\"\n",
    "VALIDATION_SOURCE = DATA_DIR / \"validation/source\"\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nData exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(directory: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    디렉토리에서 모든 JSON 파일을 로드하여 DataFrame으로 반환\n",
    "    \n",
    "    Args:\n",
    "        directory: JSON 파일이 있는 디렉토리 경로\n",
    "    \n",
    "    Returns:\n",
    "        통합된 DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    json_files = list(directory.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Loading {len(json_files)} JSON files from {directory.name}...\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Loading files\"):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # 파일명에서 카테고리 추출\n",
    "                filename = json_file.stem\n",
    "                \n",
    "                # DataFrame 생성\n",
    "                if isinstance(data, list):\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df['source_file'] = filename\n",
    "                    all_data.append(df)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file.name}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"✓ Loaded {len(combined_df):,} records\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"⚠️  No data loaded\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "training_df = load_json_files(TRAINING_LABELED)\n",
    "\n",
    "print(f\"\\nTraining data shape: {training_df.shape}\")\n",
    "print(f\"Columns: {list(training_df.columns)}\")\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING VALIDATION DATA\")\n",
    "print(\"=\"*60)\n",
    "validation_df = load_json_files(VALIDATION_LABELED)\n",
    "\n",
    "print(f\"\\nValidation data shape: {validation_df.shape}\")\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add split column\n",
    "training_df['split'] = 'Training'\n",
    "validation_df['split'] = 'Validation'\n",
    "\n",
    "# Combine datasets\n",
    "full_df = pd.concat([training_df, validation_df], ignore_index=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(full_df):,}\")\n",
    "print(f\"Training: {len(training_df):,} ({len(training_df)/len(full_df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(validation_df):,} ({len(validation_df)/len(full_df)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Data types:\")\n",
    "print(full_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 기본 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도메인 분포\n",
    "print(\"=\"*60)\n",
    "print(\"도메인 분포\")\n",
    "print(\"=\"*60)\n",
    "print(full_df['도메인'].value_counts())\n",
    "print()\n",
    "\n",
    "# 카테고리 분포\n",
    "print(\"=\"*60)\n",
    "print(\"카테고리 분포\")\n",
    "print(\"=\"*60)\n",
    "print(full_df['카테고리'].value_counts())\n",
    "print()\n",
    "\n",
    "# 화자 분포\n",
    "print(\"=\"*60)\n",
    "print(\"화자 분포\")\n",
    "print(\"=\"*60)\n",
    "print(full_df['화자'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 카테고리별 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 통계\n",
    "category_stats = full_df.groupby(['카테고리', 'split']).size().unstack(fill_value=0)\n",
    "category_stats['Total'] = category_stats.sum(axis=1)\n",
    "category_stats = category_stats.sort_values('Total', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"카테고리별 레코드 수\")\n",
    "print(\"=\"*60)\n",
    "print(category_stats)\n",
    "print()\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 카테고리별 분포\n",
    "category_counts = full_df['카테고리'].value_counts()\n",
    "axes[0].barh(range(len(category_counts)), category_counts.values)\n",
    "axes[0].set_yticks(range(len(category_counts)))\n",
    "axes[0].set_yticklabels(category_counts.index)\n",
    "axes[0].set_xlabel('레코드 수')\n",
    "axes[0].set_title('카테고리별 데이터 분포')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Train/Validation 분포\n",
    "split_counts = full_df['split'].value_counts()\n",
    "axes[1].pie(split_counts.values, labels=split_counts.index, autopct='%1.1f%%',\n",
    "           startangle=90, colors=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Train vs Validation 분포')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'category_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'category_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 대화쌍 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화셋 일련번호별 통계\n",
    "dialogue_stats = full_df.groupby('대화셋일련번호').agg({\n",
    "    '문장번호': 'count',\n",
    "    '카테고리': 'first'\n",
    "}).rename(columns={'문장번호': 'num_turns'})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"대화쌍 통계\")\n",
    "print(\"=\"*60)\n",
    "print(f\"총 대화 수: {len(dialogue_stats):,}\")\n",
    "print(f\"평균 턴 수: {dialogue_stats['num_turns'].mean():.2f}\")\n",
    "print(f\"최대 턴 수: {dialogue_stats['num_turns'].max()}\")\n",
    "print(f\"최소 턴 수: {dialogue_stats['num_turns'].min()}\")\n",
    "print()\n",
    "print(\"턴 수 분포:\")\n",
    "print(dialogue_stats['num_turns'].describe())\n",
    "\n",
    "# 턴 수 분포 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 히스토그램\n",
    "axes[0].hist(dialogue_stats['num_turns'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(dialogue_stats['num_turns'].mean(), color='red', linestyle='--',\n",
    "               label=f'평균: {dialogue_stats[\"num_turns\"].mean():.1f}')\n",
    "axes[0].set_xlabel('대화 턴 수')\n",
    "axes[0].set_ylabel('빈도')\n",
    "axes[0].set_title('대화당 턴 수 분포')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 박스플롯 (카테고리별)\n",
    "category_turns = full_df.groupby(['카테고리', '대화셋일련번호']).size().reset_index(name='turns')\n",
    "category_turns.boxplot(column='turns', by='카테고리', ax=axes[1])\n",
    "axes[1].set_xlabel('카테고리')\n",
    "axes[1].set_ylabel('턴 수')\n",
    "axes[1].set_title('카테고리별 대화 턴 수 분포')\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'dialogue_turns_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'dialogue_turns_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. QA 구조 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA 분포\n",
    "print(\"=\"*60)\n",
    "print(\"QA 구조 분석\")\n",
    "print(\"=\"*60)\n",
    "qa_dist = full_df['QA'].value_counts()\n",
    "print(qa_dist)\n",
    "print()\n",
    "\n",
    "# 화자별 QA 분포\n",
    "speaker_qa = pd.crosstab(full_df['화자'], full_df['QA'])\n",
    "print(\"화자별 QA 분포:\")\n",
    "print(speaker_qa)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# QA 분포 파이차트\n",
    "axes[0].pie(qa_dist.values, labels=qa_dist.index, autopct='%1.1f%%',\n",
    "           startangle=90)\n",
    "axes[0].set_title('질의(Q) vs 응답(A) 분포')\n",
    "\n",
    "# 화자별 QA 히트맵\n",
    "sns.heatmap(speaker_qa, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1],\n",
    "           cbar_kws={'label': '레코드 수'})\n",
    "axes[1].set_title('화자별 QA 분포')\n",
    "axes[1].set_xlabel('QA')\n",
    "axes[1].set_ylabel('화자')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'qa_structure_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'qa_structure_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 의도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객 의도 분석\n",
    "print(\"=\"*60)\n",
    "print(\"고객 의도 TOP 20\")\n",
    "print(\"=\"*60)\n",
    "customer_intent = full_df['고객의도'].value_counts().head(20)\n",
    "print(customer_intent)\n",
    "print()\n",
    "\n",
    "# 상담사 의도 분석\n",
    "print(\"=\"*60)\n",
    "print(\"상담사 의도 TOP 20\")\n",
    "print(\"=\"*60)\n",
    "agent_intent = full_df['상담사의도'].value_counts().head(20)\n",
    "print(agent_intent)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "# 고객 의도 TOP 15\n",
    "top_customer = customer_intent.head(15)\n",
    "axes[0].barh(range(len(top_customer)), top_customer.values)\n",
    "axes[0].set_yticks(range(len(top_customer)))\n",
    "axes[0].set_yticklabels(top_customer.index)\n",
    "axes[0].set_xlabel('빈도')\n",
    "axes[0].set_title('고객 의도 TOP 15')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# 상담사 의도 TOP 15\n",
    "top_agent = agent_intent.head(15)\n",
    "axes[1].barh(range(len(top_agent)), top_agent.values, color='coral')\n",
    "axes[1].set_yticks(range(len(top_agent)))\n",
    "axes[1].set_yticklabels(top_agent.index)\n",
    "axes[1].set_xlabel('빈도')\n",
    "axes[1].set_title('상담사 의도 TOP 15')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'intent_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'intent_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 텍스트 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객 질문/응답 길이\n",
    "full_df['customer_q_len'] = full_df['고객질문(요청)'].fillna('').str.len()\n",
    "full_df['customer_a_len'] = full_df['고객답변'].fillna('').str.len()\n",
    "\n",
    "# 상담사 질문/응답 길이\n",
    "full_df['agent_q_len'] = full_df['상담사질문(요청)'].fillna('').str.len()\n",
    "full_df['agent_a_len'] = full_df['상담사답변'].fillna('').str.len()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"텍스트 길이 통계\")\n",
    "print(\"=\"*60)\n",
    "length_stats = full_df[['customer_q_len', 'customer_a_len', 'agent_q_len', 'agent_a_len']].describe()\n",
    "print(length_stats)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 고객 질문 길이\n",
    "axes[0, 0].hist(full_df[full_df['customer_q_len'] > 0]['customer_q_len'],\n",
    "               bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('길이 (글자 수)')\n",
    "axes[0, 0].set_ylabel('빈도')\n",
    "axes[0, 0].set_title('고객 질문 길이 분포')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 고객 응답 길이\n",
    "axes[0, 1].hist(full_df[full_df['customer_a_len'] > 0]['customer_a_len'],\n",
    "               bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].set_xlabel('길이 (글자 수)')\n",
    "axes[0, 1].set_ylabel('빈도')\n",
    "axes[0, 1].set_title('고객 응답 길이 분포')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 상담사 질문 길이\n",
    "axes[1, 0].hist(full_df[full_df['agent_q_len'] > 0]['agent_q_len'],\n",
    "               bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_xlabel('길이 (글자 수)')\n",
    "axes[1, 0].set_ylabel('빈도')\n",
    "axes[1, 0].set_title('상담사 질문 길이 분포')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 상담사 응답 길이\n",
    "axes[1, 1].hist(full_df[full_df['agent_a_len'] > 0]['agent_a_len'],\n",
    "               bins=50, edgecolor='black', alpha=0.7, color='lightskyblue')\n",
    "axes[1, 1].set_xlabel('길이 (글자 수)')\n",
    "axes[1, 1].set_ylabel('빈도')\n",
    "axes[1, 1].set_title('상담사 응답 길이 분포')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'text_length_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 개체명/용어사전/지식베이스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개체명 분석\n",
    "print(\"=\"*60)\n",
    "print(\"개체명 분석\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 개체명이 있는 레코드 비율\n",
    "entity_ratio = (full_df['개체명 '].notna() & (full_df['개체명 '] != '')).sum() / len(full_df) * 100\n",
    "print(f\"개체명이 있는 레코드: {entity_ratio:.2f}%\")\n",
    "print()\n",
    "\n",
    "# 용어사전이 있는 레코드 비율\n",
    "term_ratio = (full_df['용어사전'].notna() & (full_df['용어사전'] != '')).sum() / len(full_df) * 100\n",
    "print(f\"용어사전이 있는 레코드: {term_ratio:.2f}%\")\n",
    "print()\n",
    "\n",
    "# 지식베이스가 있는 레코드 비율\n",
    "kb_ratio = (full_df['지식베이스'].notna() & (full_df['지식베이스'] != '')).sum() / len(full_df) * 100\n",
    "print(f\"지식베이스가 있는 레코드: {kb_ratio:.2f}%\")\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['개체명', '용어사전', '지식베이스']\n",
    "ratios = [entity_ratio, term_ratio, kb_ratio]\n",
    "\n",
    "bars = ax.bar(categories, ratios, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('비율 (%)')\n",
    "ax.set_title('개체명/용어사전/지식베이스 존재 비율')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'annotation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ 시각화 저장: {OUTPUT_DIR / 'annotation_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 샘플 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 카테고리별 샘플 대화 1개씩 출력\n",
    "print(\"=\"*60)\n",
    "print(\"카테고리별 샘플 대화\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for category in full_df['카테고리'].unique():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"카테고리: {category}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 해당 카테고리의 첫 번째 대화 선택\n",
    "    category_data = full_df[full_df['카테고리'] == category]\n",
    "    first_dialogue_id = category_data['대화셋일련번호'].iloc[0]\n",
    "    dialogue = category_data[category_data['대화셋일련번호'] == first_dialogue_id].sort_values('문장번호')\n",
    "    \n",
    "    print(f\"대화 ID: {first_dialogue_id}\")\n",
    "    print(f\"턴 수: {len(dialogue)}\\n\")\n",
    "    \n",
    "    for _, row in dialogue.head(5).iterrows():\n",
    "        speaker = \"고객\" if row['화자'] == '고객' else \"상담사\"\n",
    "        qa = row['QA']\n",
    "        \n",
    "        if qa == 'Q':\n",
    "            text = row['고객질문(요청)'] if speaker == \"고객\" else row['상담사질문(요청)']\n",
    "        else:\n",
    "            text = row['고객답변'] if speaker == \"고객\" else row['상담사답변']\n",
    "        \n",
    "        if pd.notna(text) and text:\n",
    "            print(f\"[{speaker}] {text}\")\n",
    "    \n",
    "    if len(dialogue) > 5:\n",
    "        print(f\"... (총 {len(dialogue)}턴)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 데이터 품질 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"데이터 품질 체크\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 결측값 확인\n",
    "print(\"\\n결측값:\")\n",
    "missing = full_df.isnull().sum()\n",
    "missing_pct = (missing / len(full_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Count'] > 0])\n",
    "\n",
    "# 중복 대화셋 확인\n",
    "duplicate_dialogues = full_df['대화셋일련번호'].duplicated().sum()\n",
    "print(f\"\\n중복 문장 (같은 대화셋 내): {duplicate_dialogues:,}\")\n",
    "\n",
    "# 화자별 레코드 수\n",
    "print(\"\\n화자별 레코드 수:\")\n",
    "print(full_df['화자'].value_counts())\n",
    "\n",
    "# 빈 텍스트 확인\n",
    "empty_customer_q = (full_df['고객질문(요청)'].isna() | (full_df['고객질문(요청)'] == '')).sum()\n",
    "empty_customer_a = (full_df['고객답변'].isna() | (full_df['고객답변'] == '')).sum()\n",
    "empty_agent_q = (full_df['상담사질문(요청)'].isna() | (full_df['상담사질문(요청)'] == '')).sum()\n",
    "empty_agent_a = (full_df['상담사답변'].isna() | (full_df['상담사답변'] == '')).sum()\n",
    "\n",
    "print(f\"\\n빈 텍스트:\")\n",
    "print(f\"  고객 질문: {empty_customer_q:,} ({empty_customer_q/len(full_df)*100:.1f}%)\")\n",
    "print(f\"  고객 응답: {empty_customer_a:,} ({empty_customer_a/len(full_df)*100:.1f}%)\")\n",
    "print(f\"  상담사 질문: {empty_agent_q:,} ({empty_agent_q/len(full_df)*100:.1f}%)\")\n",
    "print(f\"  상담사 응답: {empty_agent_a:,} ({empty_agent_a/len(full_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 요약 통계 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 통계 생성\n",
    "summary_stats = {\n",
    "    'total_records': len(full_df),\n",
    "    'total_dialogues': full_df['대화셋일련번호'].nunique(),\n",
    "    'training_records': len(training_df),\n",
    "    'validation_records': len(validation_df),\n",
    "    'categories': full_df['카테고리'].unique().tolist(),\n",
    "    'avg_dialogue_turns': dialogue_stats['num_turns'].mean(),\n",
    "    'entity_coverage_pct': entity_ratio,\n",
    "    'term_coverage_pct': term_ratio,\n",
    "    'kb_coverage_pct': kb_ratio,\n",
    "    'category_distribution': full_df['카테고리'].value_counts().to_dict(),\n",
    "    'speaker_distribution': full_df['화자'].value_counts().to_dict(),\n",
    "    'qa_distribution': full_df['QA'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "# JSON으로 저장\n",
    "with open(OUTPUT_DIR / 'summary_statistics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"요약 통계\")\n",
    "print(\"=\"*60)\n",
    "for key, value in summary_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"{key}: {len(value)} items\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:,}\" if isinstance(value, int) else f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\n✓ 요약 통계 저장: {OUTPUT_DIR / 'summary_statistics.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset을 CSV로 저장\n",
    "output_csv = OUTPUT_DIR / 'dasan_full_dataset.csv'\n",
    "full_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"✓ 전체 데이터셋 저장: {output_csv}\")\n",
    "print(f\"  크기: {output_csv.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# 카테고리별 요약\n",
    "category_summary = full_df.groupby('카테고리').agg({\n",
    "    '대화셋일련번호': 'nunique',\n",
    "    '문장번호': 'count',\n",
    "    'split': lambda x: f\"{(x=='Training').sum()}/{(x=='Validation').sum()}\"\n",
    "}).rename(columns={\n",
    "    '대화셋일련번호': '대화 수',\n",
    "    '문장번호': '전체 레코드',\n",
    "    'split': 'Train/Val'\n",
    "})\n",
    "\n",
    "category_summary.to_csv(OUTPUT_DIR / 'category_summary.csv', encoding='utf-8-sig')\n",
    "print(f\"✓ 카테고리 요약 저장: {OUTPUT_DIR / 'category_summary.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"카테고리별 요약\")\n",
    "print(\"=\"*60)\n",
    "print(category_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 결론 및 다음 단계\n",
    "\n",
    "### 주요 발견사항:\n",
    "- 총 레코드 수와 대화 수 확인\n",
    "- 카테고리별 데이터 분포 균형 확인\n",
    "- 대화 턴 수 및 텍스트 길이 패턴 파악\n",
    "- 개체명/용어사전/지식베이스 커버리지 분석\n",
    "\n",
    "### 데이터 품질:\n",
    "- 결측값 및 빈 텍스트 비율\n",
    "- 화자별/QA별 분포 균형\n",
    "- 의도 레이블 다양성\n",
    "\n",
    "### 다음 단계:\n",
    "1. **텍스트 전처리**: 개인정보 마스킹 검증, 특수문자 처리\n",
    "2. **토큰화 및 어휘 분석**: 형태소 분석, 주요 키워드 추출\n",
    "3. **임베딩 생성**: 문장 임베딩, 의도 임베딩 생성\n",
    "4. **모델 학습 준비**: Train/Val 분리, 배치 생성\n",
    "5. **평가 메트릭 정의**: 의도 분류, 응답 생성 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"✅ EDA 완료\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n모든 결과가 저장되었습니다: {OUTPUT_DIR}\")\n",
    "print(\"\\n생성된 파일:\")\n",
    "for file in sorted(OUTPUT_DIR.glob('*')):\n",
    "    print(f\"  - {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
